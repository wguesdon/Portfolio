{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58b6172",
   "metadata": {
    "papermill": {
     "duration": 0.004671,
     "end_time": "2025-02-08T14:17:00.614785",
     "exception": false,
     "start_time": "2025-02-08T14:17:00.610114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialise Python env and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbe00a",
   "metadata": {
    "papermill": {
     "duration": 0.003585,
     "end_time": "2025-02-08T14:17:00.622642",
     "exception": false,
     "start_time": "2025-02-08T14:17:00.619057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f44071f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:00.631983Z",
     "iopub.status.busy": "2025-02-08T14:17:00.631559Z",
     "iopub.status.idle": "2025-02-08T14:17:15.124723Z",
     "shell.execute_reply": "2025-02-08T14:17:15.123345Z"
    },
    "papermill": {
     "duration": 14.500301,
     "end_time": "2025-02-08T14:17:15.126907",
     "exception": false,
     "start_time": "2025-02-08T14:17:00.626606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "# Install packages quietly using the -q flag to suppress output\n",
    "!pip install -q latexify-py > /dev/null 2>&1\n",
    "!pip install -q openai==1.58.1 langchain-core langchain-openai > /dev/null 2>&1\n",
    "\n",
    "# Create a requirements.txt file with only our packages and their versions.\n",
    "# This command filters the pip freeze output to include just the packages of interest.\n",
    "!pip freeze | grep -E '^(latexify-py|openai|langchain-core|langchain-openai)==' > requirements.txt\n",
    "\n",
    "# Display the current Python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e63daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:15.137064Z",
     "iopub.status.busy": "2025-02-08T14:17:15.136672Z",
     "iopub.status.idle": "2025-02-08T14:17:18.007665Z",
     "shell.execute_reply": "2025-02-08T14:17:18.006392Z"
    },
    "papermill": {
     "duration": 2.878542,
     "end_time": "2025-02-08T14:17:18.009858",
     "exception": false,
     "start_time": "2025-02-08T14:17:15.131316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import latexify\n",
    "from IPython.display import display, Math, Latex\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5098949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:18.020042Z",
     "iopub.status.busy": "2025-02-08T14:17:18.019541Z",
     "iopub.status.idle": "2025-02-08T14:17:20.558024Z",
     "shell.execute_reply": "2025-02-08T14:17:20.556867Z"
    },
    "papermill": {
     "duration": 2.545533,
     "end_time": "2025-02-08T14:17:20.559871",
     "exception": false,
     "start_time": "2025-02-08T14:17:18.014338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "## LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "OPENAI_API_KEY = user_secrets.get_secret(\"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c234aff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:20.569919Z",
     "iopub.status.busy": "2025-02-08T14:17:20.569527Z",
     "iopub.status.idle": "2025-02-08T14:17:20.574244Z",
     "shell.execute_reply": "2025-02-08T14:17:20.573193Z"
    },
    "papermill": {
     "duration": 0.01161,
     "end_time": "2025-02-08T14:17:20.575931",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.564321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_LLM = 'gpt-4o-2024-05-13'\n",
    "ADVANCED_LLM = 'o3-mini'\n",
    "SELECTED_LLM = ADVANCED_LLM\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS=3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914966eb",
   "metadata": {
    "papermill": {
     "duration": 0.003812,
     "end_time": "2025-02-08T14:17:20.584029",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.580217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper function to solve math with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c29003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:20.593781Z",
     "iopub.status.busy": "2025-02-08T14:17:20.593323Z",
     "iopub.status.idle": "2025-02-08T14:17:20.601782Z",
     "shell.execute_reply": "2025-02-08T14:17:20.600566Z"
    },
    "papermill": {
     "duration": 0.015563,
     "end_time": "2025-02-08T14:17:20.603650",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.588087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve_math_problem(context, file_path='/kaggle/working/problem_solution.md'):\n",
    "    \"\"\"\n",
    "    Solve a math problem by constructing a prompt, invoking the LLM chain,\n",
    "    and saving the result to a Markdown file with a timestamp in its name.\n",
    "\n",
    "    Parameters:\n",
    "        context (str): The problem statement or context.\n",
    "        file_path (str): The base path where the Markdown report will be saved.\n",
    "                         The timestamp will be appended to this filename.\n",
    "                         Defaults to '/kaggle/working/problem_solution.md'.\n",
    "\n",
    "    Returns:\n",
    "        result (str): The generated answer in markdown format, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    # Append a timestamp to the file name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Split the file_path into the base and extension (assumes file_path has an extension)\n",
    "    base, ext = file_path.rsplit('.', 1)\n",
    "    file_path_with_timestamp = f\"{base}_{timestamp}.{ext}\"\n",
    "\n",
    "    # Define a default template with a placeholder for the context\n",
    "    template = \"\"\"\n",
    "Solve the problem below. Explain your reasoning step by step. Return the result in markdown format.\n",
    "When returning equations in the Latex format make sure to use $$ $$ around them to ensure they render in markdown.\n",
    "{context}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a ChatPromptTemplate from the template\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Prepare parameters for ChatOpenAI\n",
    "    model_params = {\n",
    "        \"model\": SELECTED_LLM,\n",
    "        \"api_key\": OPENAI_API_KEY\n",
    "    }\n",
    "    \n",
    "    display(Markdown(f'**Selected model: {SELECTED_LLM}**'))\n",
    "    \n",
    "    # Conditionally set temperature and max_tokens if the model supports these parameters\n",
    "    if SELECTED_LLM != ADVANCED_LLM:\n",
    "        model_params[\"temperature\"] = TEMPERATURE \n",
    "        model_params[\"max_tokens\"] = MAX_TOKENS\n",
    "    \n",
    "    # Initialize the ChatOpenAI model with the specified parameters\n",
    "    model = ChatOpenAI(**model_params)\n",
    "    \n",
    "    # Create the processing chain by composing the prompt, model, and output parser\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    try:\n",
    "        # Invoke the chain to generate the answer based on the context\n",
    "        result = chain.invoke(context)\n",
    "    \n",
    "        # Save both the prompt and the result to a Markdown file with a timestamp in its name\n",
    "        with open(file_path_with_timestamp, 'w') as f:\n",
    "            f.write(\"# Question\\n\\n\")\n",
    "            f.write(\"## Prompt\\n\")\n",
    "            f.write(template.format(context=context))\n",
    "            f.write(\"\\n\\n## Answer\\n\")\n",
    "            f.write(result)\n",
    "    \n",
    "        # Display the result and the file path in the notebook\n",
    "        display(Markdown(result))\n",
    "        display(Markdown(f\"**Markdown report saved to: {file_path_with_timestamp}**\"))\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    except BadRequestError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc07d26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:20.613346Z",
     "iopub.status.busy": "2025-02-08T14:17:20.612930Z",
     "iopub.status.idle": "2025-02-08T14:17:20.621252Z",
     "shell.execute_reply": "2025-02-08T14:17:20.620202Z"
    },
    "papermill": {
     "duration": 0.015185,
     "end_time": "2025-02-08T14:17:20.623082",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.607897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def solve_math_problem(context, template=None, file_path='/kaggle/working/problem_solution.md'):\n",
    "    \"\"\"\n",
    "    Solve a math problem by constructing a prompt, invoking the LLM chain,\n",
    "    and saving the result to a Markdown file with a timestamp in its name.\n",
    "\n",
    "    Parameters:\n",
    "        context (str): The problem statement or context.\n",
    "        template (str): A prompt template with a placeholder for the context.\n",
    "                        If None, a default template is used.\n",
    "        file_path (str): The base path where the Markdown report will be saved.\n",
    "                         A timestamp is appended to this filename.\n",
    "                         Defaults to '/kaggle/working/problem_solution.md'.\n",
    "\n",
    "    Returns:\n",
    "        result (str): The generated answer in markdown format, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    # Append a timestamp to the file name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Split the file_path into the base and extension (assumes file_path has an extension)\n",
    "    base, ext = file_path.rsplit('.', 1)\n",
    "    file_path_with_timestamp = f\"{base}_{timestamp}.{ext}\"\n",
    "\n",
    "    # Use the provided template or fall back to the default one\n",
    "    if template is None:\n",
    "        template = \"\"\"\n",
    "Solve the problem below. Explain your reasoning step by step. Return the result in markdown format.\n",
    "When returning equations in the Latex format make sure to use $$ $$ around them to ensure they render in markdown.\n",
    "{context}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Create a ChatPromptTemplate from the template\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Prepare parameters for ChatOpenAI\n",
    "    model_params = {\n",
    "        \"model\": SELECTED_LLM,\n",
    "        \"api_key\": OPENAI_API_KEY\n",
    "    }\n",
    "    \n",
    "    display(Markdown(f'**Selected model: {SELECTED_LLM}**'))\n",
    "    \n",
    "    # Conditionally set temperature and max_tokens if the model supports these parameters\n",
    "    if SELECTED_LLM != ADVANCED_LLM:\n",
    "        model_params[\"temperature\"] = TEMPERATURE \n",
    "        model_params[\"max_tokens\"] = MAX_TOKENS\n",
    "    \n",
    "    # Initialize the ChatOpenAI model with the specified parameters\n",
    "    model = ChatOpenAI(**model_params)\n",
    "    \n",
    "    # Create the processing chain by composing the prompt, model, and output parser\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    try:\n",
    "        # Invoke the chain to generate the answer based on the context\n",
    "        result = chain.invoke(context)\n",
    "    \n",
    "        # Save both the prompt and the result to a Markdown file with a timestamp in its name\n",
    "        with open(file_path_with_timestamp, 'w') as f:\n",
    "            f.write(\"# Question\\n\\n\")\n",
    "            f.write(\"## Prompt\\n\")\n",
    "            f.write(template.format(context=context))\n",
    "            f.write(\"\\n\\n## Answer\\n\")\n",
    "            f.write(result)\n",
    "    \n",
    "        # Display the result and the file path in the notebook\n",
    "        display(Markdown(result))\n",
    "        display(Markdown(f\"**Markdown report saved to: {file_path_with_timestamp}**\"))\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    except BadRequestError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8cd74",
   "metadata": {
    "papermill": {
     "duration": 0.003901,
     "end_time": "2025-02-08T14:17:20.631153",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.627252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beacaaf",
   "metadata": {
    "papermill": {
     "duration": 0.003766,
     "end_time": "2025-02-08T14:17:20.639054",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.635288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc30f71",
   "metadata": {
    "papermill": {
     "duration": 0.003774,
     "end_time": "2025-02-08T14:17:20.646866",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.643092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Permutations: Evaluate P(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc709b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:20.656280Z",
     "iopub.status.busy": "2025-02-08T14:17:20.655860Z",
     "iopub.status.idle": "2025-02-08T14:17:25.558211Z",
     "shell.execute_reply": "2025-02-08T14:17:25.557021Z"
    },
    "papermill": {
     "duration": 4.909558,
     "end_time": "2025-02-08T14:17:25.560465",
     "exception": false,
     "start_time": "2025-02-08T14:17:20.650907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Selected model: o3-mini**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We are asked to compute $$P(5,3)$$, which represents the number of ways to arrange 3 objects selected from 5 distinct objects.\n",
       "\n",
       "### Step-by-step Reasoning\n",
       "\n",
       "1. **Definition**: The permutation formula for selecting and arranging \\( k \\) items from \\( n \\) items is given by:\n",
       "   $$ \n",
       "   P(n, k) = \\frac{n!}{(n-k)!} \n",
       "   $$\n",
       "\n",
       "2. **Substitute the Values**: For \\( n=5 \\) and \\( k=3 \\), substitute into the formula:\n",
       "   $$ \n",
       "   P(5, 3) = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} \n",
       "   $$\n",
       "\n",
       "3. **Evaluate the Factorials**:\n",
       "   - \\( 5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120 \\)\n",
       "   - \\( 2! = 2 \\times 1 = 2 \\)\n",
       "\n",
       "4. **Compute the Final Value**:\n",
       "   $$ \n",
       "   P(5, 3) = \\frac{120}{2} = 60 \n",
       "   $$\n",
       "\n",
       "Therefore, there are **60** ways to arrange 3 items from a set of 5.\n",
       "\n",
       "---\n",
       "\n",
       "Here is a Python script that computes this result:\n",
       "\n",
       "```python\n",
       "import math\n",
       "\n",
       "# Calculate P(5,3) using the permutation formula\n",
       "result = math.factorial(5) // math.factorial(5 - 3)\n",
       "print(\"P(5,3) =\", result)\n",
       "```\n",
       "\n",
       "Alternatively, if you're using Python 3.8 or higher, you can use the `math.perm` function:\n",
       "\n",
       "```python\n",
       "import math\n",
       "\n",
       "result = math.perm(5, 3)\n",
       "print(\"P(5,3) =\", result)\n",
       "```\n",
       "\n",
       "Both scripts will output:\n",
       "\n",
       "```\n",
       "P(5,3) = 60\n",
       "```\n",
       "\n",
       "Thus, the evaluated result of \\( P(5,3) \\) is **60**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: /kaggle/working/problem_solution_20250208_141720.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SELECTED_LLM = ADVANCED_LLM\n",
    "\n",
    "my_template = \"\"\"\n",
    "Solve the problem below. Explain your reasonning step by step. Return the result in markdown format. When possible also write a Python script to solve the problem.\n",
    "Put the python code in a ```python ``` block. \n",
    "When returning equations in the Latex format make sure to use $$ $$ around them to make sure they will render in the markdown format.  \n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Define your problem context\n",
    "problem_context = \"\"\"\n",
    "Permutations: Evaluate P(5,3)\n",
    "\"\"\"\n",
    "\n",
    "# Solve the problem and get the result\n",
    "result = solve_math_problem(problem_context, my_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffc17f",
   "metadata": {
    "papermill": {
     "duration": 0.004734,
     "end_time": "2025-02-08T14:17:25.570243",
     "exception": false,
     "start_time": "2025-02-08T14:17:25.565509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47f0b2",
   "metadata": {
    "papermill": {
     "duration": 0.004418,
     "end_time": "2025-02-08T14:17:25.579176",
     "exception": false,
     "start_time": "2025-02-08T14:17:25.574758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- compute: $(^{}_6P{}_1 =)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91185c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:25.589939Z",
     "iopub.status.busy": "2025-02-08T14:17:25.589552Z",
     "iopub.status.idle": "2025-02-08T14:17:30.479026Z",
     "shell.execute_reply": "2025-02-08T14:17:30.477945Z"
    },
    "papermill": {
     "duration": 4.900374,
     "end_time": "2025-02-08T14:17:30.484392",
     "exception": false,
     "start_time": "2025-02-08T14:17:25.584018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Selected model: gpt-4o-2024-05-13**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To solve the problem of computing the permutation \\( P(6, 1) \\), we need to understand the formula for permutations. The permutation formula is given by:\n",
       "\n",
       "$$\n",
       "P(n, k) = \\frac{n!}{(n - k)!}\n",
       "$$\n",
       "\n",
       "Where:\n",
       "- \\( n \\) is the total number of items,\n",
       "- \\( k \\) is the number of items to choose,\n",
       "- \\( n! \\) denotes the factorial of \\( n \\).\n",
       "\n",
       "In this case, we have \\( n = 6 \\) and \\( k = 1 \\). Plugging these values into the formula, we get:\n",
       "\n",
       "$$\n",
       "P(6, 1) = \\frac{6!}{(6 - 1)!} = \\frac{6!}{5!}\n",
       "$$\n",
       "\n",
       "We know that \\( 6! = 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1 \\) and \\( 5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 \\). Therefore:\n",
       "\n",
       "$$\n",
       "P(6, 1) = \\frac{6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1}{5 \\times 4 \\times 3 \\times 2 \\times 1} = \\frac{6}{1} = 6\n",
       "$$\n",
       "\n",
       "So, the result of \\( P(6, 1) \\) is 6.\n",
       "\n",
       "Here is the Python code to compute this:\n",
       "\n",
       "```python\n",
       "import math\n",
       "\n",
       "def permutation(n, k):\n",
       "    return math.factorial(n) // math.factorial(n - k)\n",
       "\n",
       "result = permutation(6, 1)\n",
       "print(result)\n",
       "```\n",
       "\n",
       "This code uses the `math.factorial` function to compute the factorials and then calculates the permutation using integer division. The result will be printed as 6."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: /kaggle/working/problem_solution_20250208_141725.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SELECTED_LLM = BASE_LLM\n",
    "\n",
    "my_template = \"\"\"\n",
    "Solve the problem below. Explain your reasonning step by step. Return the result in markdown format. When possible also write a Python script to solve the problem.\n",
    "Put the python code in a ```python ``` block. \n",
    "When returning equations in the Latex format make sure to use $$ $$ around them to make sure they will render in the markdown format.  \n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Define your problem context\n",
    "problem_context = \"\"\"\n",
    "compute: $(^{}_6P{}_1 =)$\n",
    "\"\"\"\n",
    "\n",
    "# Solve the problem and get the result\n",
    "result = solve_math_problem(problem_context, my_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44d0cf",
   "metadata": {
    "papermill": {
     "duration": 0.004542,
     "end_time": "2025-02-08T14:17:30.494495",
     "exception": false,
     "start_time": "2025-02-08T14:17:30.489953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085d575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:16:08.997567Z",
     "iopub.status.busy": "2025-02-08T14:16:08.997215Z",
     "iopub.status.idle": "2025-02-08T14:16:09.004232Z",
     "shell.execute_reply": "2025-02-08T14:16:09.002896Z",
     "shell.execute_reply.started": "2025-02-08T14:16:08.997544Z"
    },
    "papermill": {
     "duration": 0.004883,
     "end_time": "2025-02-08T14:17:30.504198",
     "exception": false,
     "start_time": "2025-02-08T14:17:30.499315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Of the 7 students who are going to take a test, the top 2 will get different prizes. In how many ways can both prizes be awarded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d29e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:17:30.516180Z",
     "iopub.status.busy": "2025-02-08T14:17:30.515727Z",
     "iopub.status.idle": "2025-02-08T14:17:34.527792Z",
     "shell.execute_reply": "2025-02-08T14:17:34.526541Z"
    },
    "papermill": {
     "duration": 4.020244,
     "end_time": "2025-02-08T14:17:34.529734",
     "exception": false,
     "start_time": "2025-02-08T14:17:30.509490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Selected model: gpt-4o-2024-05-13**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To solve the problem of determining the number of ways to award the top 2 prizes to 7 students, we need to consider the following:\n",
       "\n",
       "1. **Selection of the top 2 students**: We need to choose 2 students out of 7. The order in which we choose these students matters because the prizes are different.\n",
       "\n",
       "2. **Permutations**: Since the order matters, we are dealing with permutations, not combinations.\n",
       "\n",
       "### Step-by-Step Reasoning\n",
       "\n",
       "1. **First Prize**: We have 7 choices for the first prize.\n",
       "2. **Second Prize**: After awarding the first prize, we have 6 remaining choices for the second prize.\n",
       "\n",
       "Thus, the total number of ways to award the two different prizes is given by the product of the number of choices for each prize.\n",
       "\n",
       "### Calculation\n",
       "\n",
       "The number of ways to award the prizes is:\n",
       "$$\n",
       "7 \\times 6 = 42\n",
       "$$\n",
       "\n",
       "So, there are 42 different ways to award the top 2 prizes to 7 students.\n",
       "\n",
       "### Python Script\n",
       "\n",
       "Here is a Python script to calculate the number of ways to award the prizes:\n",
       "\n",
       "```python\n",
       "def calculate_ways_to_award_prizes(total_students, prizes):\n",
       "    ways = 1\n",
       "    for i in range(prizes):\n",
       "        ways *= (total_students - i)\n",
       "    return ways\n",
       "\n",
       "total_students = 7\n",
       "prizes = 2\n",
       "ways_to_award_prizes = calculate_ways_to_award_prizes(total_students, prizes)\n",
       "print(f\"The number of ways to award the top 2 prizes to {total_students} students is: {ways_to_award_prizes}\")\n",
       "```\n",
       "\n",
       "This script defines a function `calculate_ways_to_award_prizes` that calculates the number of ways to award the prizes by multiplying the number of choices for each prize. It then prints the result.\n",
       "\n",
       "### Result\n",
       "\n",
       "The number of ways to award the top 2 prizes to 7 students is:\n",
       "$$\n",
       "42\n",
       "$$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: /kaggle/working/problem_solution_20250208_141730.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SELECTED_LLM = BASE_LLM\n",
    "\n",
    "# Define your problem context\n",
    "problem_context = \"\"\"\n",
    "Of the 7 students who are going to take a test, the top 2 will get different prizes. In how many ways can both prizes be awarded?\n",
    "\"\"\"\n",
    "\n",
    "# Solve the problem and get the result\n",
    "result = solve_math_problem(problem_context, my_template)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.689105,
   "end_time": "2025-02-08T14:17:35.456760",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-08T14:16:57.767655",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
