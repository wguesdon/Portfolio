{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769202e3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T12:16:55.693748Z",
     "iopub.status.busy": "2024-11-13T12:16:55.693175Z",
     "iopub.status.idle": "2024-11-13T21:55:47.809304Z",
     "shell.execute_reply": "2024-11-13T21:55:47.807691Z"
    },
    "papermill": {
     "duration": 34732.124629,
     "end_time": "2024-11-13T21:55:47.812532",
     "exception": false,
     "start_time": "2024-11-13T12:16:55.687903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:17:03,265] A new study created in memory with name: no-name-ccee4db9-d353-426b-b70d-b219952e18ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.160791\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.159835\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.155851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:17:17,887] Trial 0 finished with value: 0.9366950959488273 and parameters: {'n_estimators': 476, 'num_leaves': 66, 'learning_rate': 0.1833218914547814, 'min_child_samples': 31, 'subsample': 0.883142706467505}. Best is trial 0 with value: 0.9366950959488273.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[179]\tvalid_0's binary_logloss: 0.166161\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[179]\tvalid_0's binary_logloss: 0.164866\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[179]\tvalid_0's binary_logloss: 0.161798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:17:39,415] Trial 1 finished with value: 0.9375408670931059 and parameters: {'n_estimators': 179, 'num_leaves': 62, 'learning_rate': 0.017528844084858095, 'min_child_samples': 26, 'subsample': 0.9739808361617948}. Best is trial 1 with value: 0.9375408670931059.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's binary_logloss: 0.159954\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.158939\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.155552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:17:58,586] Trial 2 finished with value: 0.937498223169865 and parameters: {'n_estimators': 167, 'num_leaves': 93, 'learning_rate': 0.12280902810592578, 'min_child_samples': 34, 'subsample': 0.5975023415837712}. Best is trial 1 with value: 0.9375408670931059.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.16297\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.162181\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:18:14,097] Trial 3 finished with value: 0.9364605543710022 and parameters: {'n_estimators': 594, 'num_leaves': 88, 'learning_rate': 0.2383540182973698, 'min_child_samples': 9, 'subsample': 0.5028601163884551}. Best is trial 1 with value: 0.9375408670931059.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.157903\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.157356\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid_0's binary_logloss: 0.157741\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's binary_logloss: 0.153304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:18:41,710] Trial 4 finished with value: 0.9377683013503909 and parameters: {'n_estimators': 738, 'num_leaves': 58, 'learning_rate': 0.058665406319911975, 'min_child_samples': 38, 'subsample': 0.925368339112632}. Best is trial 4 with value: 0.9377683013503909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.157832\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's binary_logloss: 0.157692\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's binary_logloss: 0.154085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:18:59,738] Trial 5 finished with value: 0.937590618336887 and parameters: {'n_estimators': 835, 'num_leaves': 43, 'learning_rate': 0.11043127926484399, 'min_child_samples': 7, 'subsample': 0.8929522176299234}. Best is trial 4 with value: 0.9377683013503909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.158548\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.158268\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's binary_logloss: 0.154092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:19:23,391] Trial 6 finished with value: 0.93727078891258 and parameters: {'n_estimators': 188, 'num_leaves': 78, 'learning_rate': 0.07245006715574497, 'min_child_samples': 50, 'subsample': 0.7771385784352492}. Best is trial 4 with value: 0.9377683013503909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's binary_logloss: 0.158313\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's binary_logloss: 0.158024\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's binary_logloss: 0.153669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:19:56,610] Trial 7 finished with value: 0.937498223169865 and parameters: {'n_estimators': 407, 'num_leaves': 91, 'learning_rate': 0.03968573585284045, 'min_child_samples': 28, 'subsample': 0.9512773645727439}. Best is trial 4 with value: 0.9377683013503909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[376]\tvalid_0's binary_logloss: 0.155517\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[345]\tvalid_0's binary_logloss: 0.155677\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[340]\tvalid_0's binary_logloss: 0.151287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:20:36,447] Trial 8 finished with value: 0.9382942430703625 and parameters: {'n_estimators': 389, 'num_leaves': 22, 'learning_rate': 0.027305208961937385, 'min_child_samples': 30, 'subsample': 0.9967126337962093}. Best is trial 8 with value: 0.9382942430703625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.16011\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.159822\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.156033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:20:55,550] Trial 9 finished with value: 0.9372423596304194 and parameters: {'n_estimators': 771, 'num_leaves': 126, 'learning_rate': 0.1388954450320937, 'min_child_samples': 47, 'subsample': 0.7039224060824176}. Best is trial 8 with value: 0.9382942430703625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[349]\tvalid_0's binary_logloss: 0.164186\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[349]\tvalid_0's binary_logloss: 0.162944\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[349]\tvalid_0's binary_logloss: 0.159655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:21:31,341] Trial 10 finished with value: 0.9372992181947406 and parameters: {'n_estimators': 349, 'num_leaves': 27, 'learning_rate': 0.010836864218003754, 'min_child_samples': 18, 'subsample': 0.7831650173712091}. Best is trial 8 with value: 0.9382942430703625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[231]\tvalid_0's binary_logloss: 0.155533\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's binary_logloss: 0.155953\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[227]\tvalid_0's binary_logloss: 0.151528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:22:03,657] Trial 11 finished with value: 0.9380099502487562 and parameters: {'n_estimators': 630, 'num_leaves': 23, 'learning_rate': 0.04143779884878618, 'min_child_samples': 40, 'subsample': 0.9973030557945183}. Best is trial 8 with value: 0.9382942430703625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[414]\tvalid_0's binary_logloss: 0.155122\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[407]\tvalid_0's binary_logloss: 0.155332\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[394]\tvalid_0's binary_logloss: 0.15121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:22:52,430] Trial 12 finished with value: 0.9384008528784649 and parameters: {'n_estimators': 971, 'num_leaves': 20, 'learning_rate': 0.024639430998757615, 'min_child_samples': 41, 'subsample': 0.837124717086883}. Best is trial 12 with value: 0.9384008528784649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[335]\tvalid_0's binary_logloss: 0.156626\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's binary_logloss: 0.156406\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[339]\tvalid_0's binary_logloss: 0.152377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:23:41,462] Trial 13 finished with value: 0.9379246624022743 and parameters: {'n_estimators': 989, 'num_leaves': 40, 'learning_rate': 0.023254018027190716, 'min_child_samples': 21, 'subsample': 0.8303807284245661}. Best is trial 12 with value: 0.9384008528784649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[216]\tvalid_0's binary_logloss: 0.158053\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17045, number of negative: 76755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's binary_logloss: 0.157882\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17044, number of negative: 76756\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 93800, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181706 -> initscore=-1.504833\n",
      "[LightGBM] [Info] Start training from score -1.504833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[231]\tvalid_0's binary_logloss: 0.154027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:24:27,302] Trial 14 finished with value: 0.9377185501066098 and parameters: {'n_estimators': 968, 'num_leaves': 145, 'learning_rate': 0.026045706393390454, 'min_child_samples': 43, 'subsample': 0.675576851639372}. Best is trial 12 with value: 0.9384008528784649.\n",
      "[I 2024-11-13 12:24:27,305] A new study created in memory with name: no-name-48fb76e0-0fcb-4f26-b0b9-78cac1418c1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for LightGBM: {'n_estimators': 971, 'num_leaves': 20, 'learning_rate': 0.024639430998757615, 'min_child_samples': 41, 'subsample': 0.837124717086883}\n",
      "\n",
      "Tuning CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 12:28:38,198] Trial 0 finished with value: 0.9369722814498934 and parameters: {'iterations': 821, 'depth': 6, 'learning_rate': 0.012179959702313095, 'l2_leaf_reg': 0.04276463637189424, 'random_strength': 10.839807682662931}. Best is trial 0 with value: 0.9369722814498934.\n",
      "[I 2024-11-13 12:30:06,865] Trial 1 finished with value: 0.9301137171286425 and parameters: {'iterations': 280, 'depth': 6, 'learning_rate': 0.014323486114627648, 'l2_leaf_reg': 0.09801798572705687, 'random_strength': 17.781804354542146}. Best is trial 0 with value: 0.9369722814498934.\n",
      "[I 2024-11-13 12:33:51,102] Trial 2 finished with value: 0.9388343994314143 and parameters: {'iterations': 573, 'depth': 7, 'learning_rate': 0.019583662827699706, 'l2_leaf_reg': 0.00388112710472163, 'random_strength': 3.5919341139814627}. Best is trial 2 with value: 0.9388343994314143.\n",
      "[I 2024-11-13 12:37:00,499] Trial 3 finished with value: 0.9356076759061833 and parameters: {'iterations': 411, 'depth': 9, 'learning_rate': 0.015003202490413002, 'l2_leaf_reg': 0.9912347003790639, 'random_strength': 12.457496321153839}. Best is trial 2 with value: 0.9388343994314143.\n",
      "[I 2024-11-13 12:37:24,061] Trial 4 finished with value: 0.9247903340440654 and parameters: {'iterations': 166, 'depth': 10, 'learning_rate': 0.011271414334055456, 'l2_leaf_reg': 0.42454399269498294, 'random_strength': 13.006269806300736}. Best is trial 2 with value: 0.9388343994314143.\n",
      "Training has stopped (degenerate solution on iteration 110, probably too small l2-regularization, try to increase it)\n",
      "[I 2024-11-13 12:39:33,180] Trial 5 finished with value: 0.9384861407249467 and parameters: {'iterations': 900, 'depth': 10, 'learning_rate': 0.113321124639173, 'l2_leaf_reg': 0.0010643216718415275, 'random_strength': 3.398899244671598}. Best is trial 2 with value: 0.9388343994314143.\n",
      "[I 2024-11-13 12:40:52,134] Trial 6 finished with value: 0.9399715707178393 and parameters: {'iterations': 857, 'depth': 5, 'learning_rate': 0.28569741642557295, 'l2_leaf_reg': 0.06341815841556608, 'random_strength': 7.19129656774867}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:42:13,778] Trial 7 finished with value: 0.9392395167022033 and parameters: {'iterations': 594, 'depth': 8, 'learning_rate': 0.24760658127868632, 'l2_leaf_reg': 0.15892196951976234, 'random_strength': 2.170073487078159}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:43:55,969] Trial 8 finished with value: 0.9399147121535181 and parameters: {'iterations': 424, 'depth': 4, 'learning_rate': 0.06620370422118216, 'l2_leaf_reg': 0.4591120575211518, 'random_strength': 3.021243399059727}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:46:02,433] Trial 9 finished with value: 0.939957356076759 and parameters: {'iterations': 798, 'depth': 4, 'learning_rate': 0.1531321742525501, 'l2_leaf_reg': 0.03050833634753441, 'random_strength': 10.928650839689556}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:49:28,955] Trial 10 finished with value: 0.9398081023454158 and parameters: {'iterations': 710, 'depth': 5, 'learning_rate': 0.042079266850876845, 'l2_leaf_reg': 4.409110726331625, 'random_strength': 7.359131981838971}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:50:46,433] Trial 11 finished with value: 0.9396588486140725 and parameters: {'iterations': 984, 'depth': 4, 'learning_rate': 0.28903197355113075, 'l2_leaf_reg': 0.016726411407133338, 'random_strength': 7.801735582568503}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:52:37,396] Trial 12 finished with value: 0.9399147121535181 and parameters: {'iterations': 759, 'depth': 5, 'learning_rate': 0.16181772183292234, 'l2_leaf_reg': 0.02256604500205189, 'random_strength': 15.352142783049207}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:54:42,296] Trial 13 finished with value: 0.9399360341151386 and parameters: {'iterations': 992, 'depth': 4, 'learning_rate': 0.14035566033370261, 'l2_leaf_reg': 0.007487339568765881, 'random_strength': 6.937354074061197}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:57:56,365] Trial 14 finished with value: 0.9398791755508173 and parameters: {'iterations': 658, 'depth': 5, 'learning_rate': 0.07480796584968362, 'l2_leaf_reg': 0.06833154557794927, 'random_strength': 9.172194215527647}. Best is trial 6 with value: 0.9399715707178393.\n",
      "[I 2024-11-13 12:57:56,368] A new study created in memory with name: no-name-e578601f-f91a-4f33-a421-7be6c0735e02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for CatBoost: {'iterations': 857, 'depth': 5, 'learning_rate': 0.28569741642557295, 'l2_leaf_reg': 0.06341815841556608, 'random_strength': 7.19129656774867}\n",
      "\n",
      "Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 13:26:33,799] Trial 0 finished with value: 0.9393461265103056 and parameters: {'n_estimators': 429, 'max_depth': 10, 'learning_rate': 0.027287957478457112, 'subsample': 0.7406349519396822, 'colsample_bytree': 0.587878878379606}. Best is trial 0 with value: 0.9393461265103056.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 13:39:07,932] Trial 1 finished with value: 0.9385856432125089 and parameters: {'n_estimators': 169, 'max_depth': 8, 'learning_rate': 0.03004582318848185, 'subsample': 0.789573312755669, 'colsample_bytree': 0.8787558332875016}. Best is trial 0 with value: 0.9393461265103056.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 13:51:22,643] Trial 2 finished with value: 0.9384434968017058 and parameters: {'n_estimators': 446, 'max_depth': 10, 'learning_rate': 0.13586983370956826, 'subsample': 0.8506205367683435, 'colsample_bytree': 0.6909443776019566}. Best is trial 0 with value: 0.9393461265103056.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 14:36:47,820] Trial 3 finished with value: 0.9394669509594883 and parameters: {'n_estimators': 710, 'max_depth': 7, 'learning_rate': 0.013546329667785817, 'subsample': 0.7515650189374289, 'colsample_bytree': 0.7788279048687582}. Best is trial 3 with value: 0.9394669509594883.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 14:53:21,227] Trial 4 finished with value: 0.9392963752665245 and parameters: {'n_estimators': 584, 'max_depth': 8, 'learning_rate': 0.08380500503696882, 'subsample': 0.6452606582563174, 'colsample_bytree': 0.9576583267218355}. Best is trial 3 with value: 0.9394669509594883.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 15:46:56,251] Trial 5 finished with value: 0.9399715707178393 and parameters: {'n_estimators': 863, 'max_depth': 7, 'learning_rate': 0.023768296979715303, 'subsample': 0.9151884023656969, 'colsample_bytree': 0.5449558021762164}. Best is trial 5 with value: 0.9399715707178393.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 16:11:53,350] Trial 6 finished with value: 0.939680170575693 and parameters: {'n_estimators': 635, 'max_depth': 5, 'learning_rate': 0.09489925861715254, 'subsample': 0.8676167628478803, 'colsample_bytree': 0.9106197296306546}. Best is trial 5 with value: 0.9399715707178393.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 16:20:03,391] Trial 7 finished with value: 0.9352523098791755 and parameters: {'n_estimators': 107, 'max_depth': 4, 'learning_rate': 0.03877675997599556, 'subsample': 0.881205570520724, 'colsample_bytree': 0.5600513085473983}. Best is trial 5 with value: 0.9399715707178393.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 16:38:04,776] Trial 8 finished with value: 0.9388770433546553 and parameters: {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.02859697221369162, 'subsample': 0.9432799491433483, 'colsample_bytree': 0.9668762130181148}. Best is trial 5 with value: 0.9399715707178393.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 17:27:32,379] Trial 9 finished with value: 0.9400568585643212 and parameters: {'n_estimators': 999, 'max_depth': 5, 'learning_rate': 0.03797952254718775, 'subsample': 0.624820499801507, 'colsample_bytree': 0.6916170018267218}. Best is trial 9 with value: 0.9400568585643212.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 17:40:17,592] Trial 10 finished with value: 0.9387562189054727 and parameters: {'n_estimators': 993, 'max_depth': 5, 'learning_rate': 0.20898602805926572, 'subsample': 0.522700001366629, 'colsample_bytree': 0.6976096878485247}. Best is trial 9 with value: 0.9400568585643212.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 18:42:37,916] Trial 11 finished with value: 0.9395593461265103 and parameters: {'n_estimators': 988, 'max_depth': 6, 'learning_rate': 0.012199646807132397, 'subsample': 0.6170990573510282, 'colsample_bytree': 0.504585955972834}. Best is trial 9 with value: 0.9400568585643212.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 19:30:46,701] Trial 12 finished with value: 0.9399857853589196 and parameters: {'n_estimators': 829, 'max_depth': 4, 'learning_rate': 0.054444718587903215, 'subsample': 0.979454182017579, 'colsample_bytree': 0.6624174496661047}. Best is trial 9 with value: 0.9400568585643212.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 20:12:16,142] Trial 13 finished with value: 0.9397157071783937 and parameters: {'n_estimators': 821, 'max_depth': 4, 'learning_rate': 0.0706068687262293, 'subsample': 0.980180619847056, 'colsample_bytree': 0.646264894083435}. Best is trial 9 with value: 0.9400568585643212.\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 20:54:44,334] Trial 14 finished with value: 0.9399644633972992 and parameters: {'n_estimators': 837, 'max_depth': 5, 'learning_rate': 0.04874473050292543, 'subsample': 0.608479617753721, 'colsample_bytree': 0.7822811964760155}. Best is trial 9 with value: 0.9400568585643212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for XGBoost: {'n_estimators': 999, 'max_depth': 5, 'learning_rate': 0.03797952254718775, 'subsample': 0.624820499801507, 'colsample_bytree': 0.6916170018267218}\n",
      "\n",
      "Training models with tuned parameters...\n",
      "Training LightGBM - Fold 1/5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 20454, number of negative: 92106\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 112560, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's binary_logloss: 0.152173\n",
      "LightGBM Fold 1 accuracy: 0.9381\n",
      "Training LightGBM - Fold 2/5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 20454, number of negative: 92106\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 112560, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[406]\tvalid_0's binary_logloss: 0.156956\n",
      "LightGBM Fold 2 accuracy: 0.9377\n",
      "Training LightGBM - Fold 3/5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 20454, number of negative: 92106\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 112560, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181716 -> initscore=-1.504762\n",
      "[LightGBM] [Info] Start training from score -1.504762\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[485]\tvalid_0's binary_logloss: 0.152345\n",
      "LightGBM Fold 3 accuracy: 0.9403\n",
      "Training LightGBM - Fold 4/5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 20453, number of negative: 92107\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 112560, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181708 -> initscore=-1.504821\n",
      "[LightGBM] [Info] Start training from score -1.504821\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[440]\tvalid_0's binary_logloss: 0.148328\n",
      "LightGBM Fold 4 accuracy: 0.9406\n",
      "Training LightGBM - Fold 5/5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 20453, number of negative: 92107\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 695\n",
      "[LightGBM] [Info] Number of data points in the train set: 112560, number of used features: 18\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.181708 -> initscore=-1.504821\n",
      "[LightGBM] [Info] Start training from score -1.504821\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[442]\tvalid_0's binary_logloss: 0.153506\n",
      "LightGBM Fold 5 accuracy: 0.9381\n",
      "Training CatBoost - Fold 1/5\n",
      "CatBoost Fold 1 accuracy: 0.9394\n",
      "Training CatBoost - Fold 2/5\n",
      "CatBoost Fold 2 accuracy: 0.9391\n",
      "Training CatBoost - Fold 3/5\n",
      "CatBoost Fold 3 accuracy: 0.9400\n",
      "Training CatBoost - Fold 4/5\n",
      "CatBoost Fold 4 accuracy: 0.9414\n",
      "Training CatBoost - Fold 5/5\n",
      "CatBoost Fold 5 accuracy: 0.9394\n",
      "Training XGBoost - Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Fold 1 accuracy: 0.9400\n",
      "Training XGBoost - Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Fold 2 accuracy: 0.9388\n",
      "Training XGBoost - Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Fold 3 accuracy: 0.9409\n",
      "Training XGBoost - Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Fold 4 accuracy: 0.9414\n",
      "Training XGBoost - Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Fold 5 accuracy: 0.9389\n",
      "\n",
      "Training meta model...\n",
      "\n",
      "Final ensemble submission file has been saved.\n",
      "\n",
      "Sample of final predictions:\n",
      "       id  Depression\n",
      "0  140700           0\n",
      "1  140701           0\n",
      "2  140702           0\n",
      "3  140703           1\n",
      "4  140704           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Constants\n",
    "ENV = 'Kaggle'  # Set to 'Colab', 'Kaggle', or 'Sagemaker'\n",
    "DEV = False  # Set to True to enable subsetting, False for full training data\n",
    "SUBSET_SIZE = 1000  # Number of samples for the subset during development\n",
    "TRIALS = 15  # Number of trials for Optuna\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Basic setup\n",
    "ID_COL = 'id'\n",
    "TARGET_COL = 'Depression'\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load data based on environment setting.\"\"\"\n",
    "    paths = {\n",
    "        'Kaggle': '/kaggle/input/playground-series-s4e11/',\n",
    "        'Sagemaker': '/home/ec2-user/SageMaker/data/PS4E11/',\n",
    "        'Colab': '/content/drive/MyDrive/Kaggle_analysis/PS4E11/data/'\n",
    "    }\n",
    "    base_path = paths.get(ENV)\n",
    "    if not base_path:\n",
    "        raise ValueError(\"Invalid environment specified\")\n",
    "    \n",
    "    train_data = pd.read_csv(base_path + 'train.csv')\n",
    "    test_data = pd.read_csv(base_path + 'test.csv')\n",
    "    sample_submission = pd.read_csv(base_path + 'sample_submission.csv')\n",
    "    \n",
    "    return train_data, test_data, sample_submission\n",
    "\n",
    "def preprocess_data(train_data, test_data):\n",
    "    \"\"\"Preprocess the data for model training.\"\"\"\n",
    "    X = train_data.drop(columns=[ID_COL, TARGET_COL])\n",
    "    y = train_data[TARGET_COL]\n",
    "    X_test = test_data.drop(columns=[ID_COL])\n",
    "\n",
    "    if DEV:\n",
    "        subset_indices = np.random.choice(X.index, size=min(SUBSET_SIZE, len(X)), replace=False)\n",
    "        X = X.loc[subset_indices]\n",
    "        y = y.loc[subset_indices]\n",
    "        X_test = X_test.iloc[:SUBSET_SIZE]\n",
    "\n",
    "    # Handle categorical features\n",
    "    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    X[cat_features] = X[cat_features].fillna('missing')\n",
    "    X_test[cat_features] = X_test[cat_features].fillna('missing')\n",
    "\n",
    "    # Convert categorical features to category dtype\n",
    "    for col in cat_features:\n",
    "        X[col] = X[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "    # One-hot encode for XGBoost\n",
    "    X_encoded = pd.get_dummies(X, columns=cat_features)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=cat_features)\n",
    "\n",
    "    # Align columns\n",
    "    X_encoded, X_test_encoded = X_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    return X, y, X_test, X_encoded, X_test_encoded, cat_features\n",
    "\n",
    "def get_model(model_type, params):\n",
    "    \"\"\"Create model instance based on type.\"\"\"\n",
    "    if model_type == 'CatBoost':\n",
    "        return CatBoostClassifier(**params)\n",
    "    elif model_type == 'XGBoost':\n",
    "        return XGBClassifier(**params)\n",
    "    elif model_type == 'LightGBM':\n",
    "        return LGBMClassifier(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "def objective(trial, model_type, X, y, cat_features=None, X_encoded=None):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "\n",
    "    # Suggest hyperparameters based on model type\n",
    "    if model_type == 'CatBoost':\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1, 20),\n",
    "            'verbose': 0,\n",
    "        }\n",
    "    elif model_type == 'XGBoost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'verbosity': 0,\n",
    "        }\n",
    "    elif model_type == 'LightGBM':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        if model_type == 'XGBoost':\n",
    "            X_train, X_valid = X_encoded.iloc[train_idx], X_encoded.iloc[valid_idx]\n",
    "\n",
    "        model = get_model(model_type, params)\n",
    "        \n",
    "        try:\n",
    "            if model_type == 'CatBoost':\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    cat_features=cat_features,\n",
    "                    eval_set=(X_valid, y_valid),\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif model_type == 'XGBoost':\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:  # LightGBM\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "                )\n",
    "\n",
    "            oof_preds[valid_idx] = model.predict(X_valid)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model fitting: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return accuracy_score(y, oof_preds)\n",
    "\n",
    "def tune_hyperparameters(X, y, model_type, cat_features=None, X_encoded=None):\n",
    "    \"\"\"Run Optuna study to find best hyperparameters.\"\"\"\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, model_type, X, y, cat_features, X_encoded), n_trials=TRIALS)\n",
    "    print(f\"Best hyperparameters for {model_type}: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_and_predict_with_tuned_params(X, y, X_test, model_type, cat_features, params, X_encoded=None, X_test_encoded=None):\n",
    "    \"\"\"Train model with tuned parameters and make predictions.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"Training {model_type} - Fold {fold}/5\")\n",
    "        \n",
    "        if model_type == 'XGBoost':\n",
    "            X_train, X_valid = X_encoded.iloc[train_idx], X_encoded.iloc[valid_idx]\n",
    "            X_test_current = X_test_encoded\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            X_test_current = X_test\n",
    "\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model = get_model(model_type, params)\n",
    "        \n",
    "        try:\n",
    "            if model_type == 'CatBoost':\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    cat_features=cat_features,\n",
    "                    eval_set=(X_valid, y_valid),\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif model_type == 'XGBoost':\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:  # LightGBM\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "                )\n",
    "\n",
    "            oof_preds[valid_idx] = model.predict(X_valid)\n",
    "            test_preds += model.predict(X_test_current) / skf.n_splits\n",
    "\n",
    "            # Print fold performance\n",
    "            fold_score = accuracy_score(y_valid, model.predict(X_valid))\n",
    "            print(f\"{model_type} Fold {fold} accuracy: {fold_score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fold {fold}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return oof_preds, test_preds\n",
    "\n",
    "def train_meta_model(oof_preds_list, y):\n",
    "    \"\"\"Train meta-model using base model predictions.\"\"\"\n",
    "    X_meta = np.column_stack(oof_preds_list)\n",
    "    meta_model = LogisticRegression()\n",
    "    meta_model.fit(X_meta, y)\n",
    "    return meta_model\n",
    "\n",
    "def generate_final_predictions(meta_model, test_preds_list):\n",
    "    \"\"\"Generate ensemble predictions using meta-model.\"\"\"\n",
    "    X_meta_test = np.column_stack(test_preds_list)\n",
    "    final_preds = meta_model.predict(X_meta_test)\n",
    "    return final_preds\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_data, test_data, sample_submission = load_data()\n",
    "X, y, X_test, X_encoded, X_test_encoded, cat_features = preprocess_data(train_data, test_data)\n",
    "\n",
    "# Tune and train models\n",
    "print(\"\\nTuning LightGBM...\")\n",
    "best_params_lgbm = tune_hyperparameters(X, y, 'LightGBM', cat_features)\n",
    "\n",
    "print(\"\\nTuning CatBoost...\")\n",
    "best_params_catboost = tune_hyperparameters(X, y, 'CatBoost', cat_features)\n",
    "\n",
    "print(\"\\nTuning XGBoost...\")\n",
    "best_params_xgboost = tune_hyperparameters(X, y, 'XGBoost', None, X_encoded)\n",
    "\n",
    "# Train models and get predictions\n",
    "print(\"\\nTraining models with tuned parameters...\")\n",
    "oof_lgbm, test_lgbm = train_and_predict_with_tuned_params(\n",
    "    X, y, X_test, 'LightGBM', cat_features, best_params_lgbm\n",
    ")\n",
    "oof_catboost, test_catboost = train_and_predict_with_tuned_params(\n",
    "    X, y, X_test, 'CatBoost', cat_features, best_params_catboost\n",
    ")\n",
    "oof_xgboost, test_xgboost = train_and_predict_with_tuned_params(\n",
    "    X, y, X_test, 'XGBoost', None, best_params_xgboost, X_encoded, X_test_encoded\n",
    ")\n",
    "\n",
    "# Train meta model and generate final predictions\n",
    "print(\"\\nTraining meta model...\")\n",
    "oof_preds_list = [oof_catboost, oof_xgboost, oof_lgbm]\n",
    "test_preds_list = [test_catboost, test_xgboost, test_lgbm]\n",
    "\n",
    "meta_model = train_meta_model(oof_preds_list, y)\n",
    "final_preds = generate_final_predictions(meta_model, test_preds_list)\n",
    "\n",
    "# Create and save submission\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_data[ID_COL],\n",
    "    TARGET_COL: final_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nFinal ensemble submission file has been saved.\")\n",
    "\n",
    "# Display sample of predictions\n",
    "print(\"\\nSample of final predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10008389,
     "sourceId": 84895,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34736.613378,
   "end_time": "2024-11-13T21:55:49.064369",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-13T12:16:52.450991",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
