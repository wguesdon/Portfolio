{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d71694b",
   "metadata": {
    "papermill": {
     "duration": 0.004709,
     "end_time": "2025-01-12T00:06:28.847704",
     "exception": false,
     "start_time": "2025-01-12T00:06:28.842995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!-- PS-S5.E01 -->\n",
    "\n",
    "<div style=\"font-family: 'Poppins'; font-weight: bold; letter-spacing: 0px; color: #FFFFFF; font-size: 500%; text-align: center; padding: 15px; background: #0A0F29; border: 8px solid #00FFFF; border-radius: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5);\">\n",
    "    LLM : EDA and code generation <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bb289",
   "metadata": {
    "papermill": {
     "duration": 0.003514,
     "end_time": "2025-01-12T00:06:28.855650",
     "exception": false,
     "start_time": "2025-01-12T00:06:28.852136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- simple example on how to use LLM for:\n",
    "    - generating EDA summaries\n",
    "    - generating code for initial baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6df9e0",
   "metadata": {
    "papermill": {
     "duration": 0.003532,
     "end_time": "2025-01-12T00:06:28.862900",
     "exception": false,
     "start_time": "2025-01-12T00:06:28.859368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Next steps:\n",
    "    - Include agents in the workflow\n",
    "    - Pass along a summary of the competition instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b842e6f",
   "metadata": {
    "papermill": {
     "duration": 0.003491,
     "end_time": "2025-01-12T00:06:28.870037",
     "exception": false,
     "start_time": "2025-01-12T00:06:28.866546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"background-color:#0A0F29; font-family:'Poppins', bold; color:#E0F7FA; font-size:140%; text-align:center; border: 2px solid #00FFFF; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">Generate an EDA summary</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9d75d0",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:28.878577Z",
     "iopub.status.busy": "2025-01-12T00:06:28.878195Z",
     "iopub.status.idle": "2025-01-12T00:06:38.735602Z",
     "shell.execute_reply": "2025-01-12T00:06:38.734373Z"
    },
    "papermill": {
     "duration": 9.863933,
     "end_time": "2025-01-12T00:06:38.737602",
     "exception": false,
     "start_time": "2025-01-12T00:06:28.873669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.58.1\r\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting langchain-core\r\n",
      "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting langchain-openai\r\n",
      "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (3.7.1)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.58.1) (1.7.0)\r\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.58.1)\r\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.58.1)\r\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (2.9.2)\r\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (4.66.5)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (4.12.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting langsmith<0.3,>=0.1.125 (from langchain-core)\r\n",
      "  Downloading langsmith-0.2.10-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.58.1) (3.10)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.58.1) (1.2.2)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.58.1) (2024.8.30)\r\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.58.1)\r\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\r\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.58.1)\r\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.125->langchain-core)\r\n",
      "  Downloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core) (2.32.3)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.125->langchain-core)\r\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.58.1) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.58.1) (2.23.4)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core) (2.2.3)\r\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_openai-0.3.0-py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading langsmith-0.2.10-py3-none-any.whl (326 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: orjson, jsonpatch, jiter, h11, requests-toolbelt, httpcore, httpx, openai, langsmith, langchain-core, langchain-openai\r\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 jsonpatch-1.33 langchain-core-0.3.29 langchain-openai-0.3.0 langsmith-0.2.10 openai-1.58.1 orjson-3.10.14 requests-toolbelt-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.58.1 langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834a0fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:38.750657Z",
     "iopub.status.busy": "2025-01-12T00:06:38.750291Z",
     "iopub.status.idle": "2025-01-12T00:06:38.754585Z",
     "shell.execute_reply": "2025-01-12T00:06:38.753692Z"
    },
    "papermill": {
     "duration": 0.012588,
     "end_time": "2025-01-12T00:06:38.756373",
     "exception": false,
     "start_time": "2025-01-12T00:06:38.743785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_LLM = 'gpt-4o-2024-05-13'\n",
    "ADVANCED_LLM = 'o1-preview'\n",
    "SELECTED_LLM = BASE_LLM\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c31ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:38.769287Z",
     "iopub.status.busy": "2025-01-12T00:06:38.768897Z",
     "iopub.status.idle": "2025-01-12T00:06:40.426683Z",
     "shell.execute_reply": "2025-01-12T00:06:40.425529Z"
    },
    "papermill": {
     "duration": 1.666213,
     "end_time": "2025-01-12T00:06:40.428515",
     "exception": false,
     "start_time": "2025-01-12T00:06:38.762302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "## LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea453d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:40.443576Z",
     "iopub.status.busy": "2025-01-12T00:06:40.443061Z",
     "iopub.status.idle": "2025-01-12T00:06:40.732131Z",
     "shell.execute_reply": "2025-01-12T00:06:40.731251Z"
    },
    "papermill": {
     "duration": 0.297847,
     "end_time": "2025-01-12T00:06:40.733887",
     "exception": false,
     "start_time": "2025-01-12T00:06:40.436040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "OPENAI_API_KEY = user_secrets.get_secret(\"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab7bab9",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:40.746373Z",
     "iopub.status.busy": "2025-01-12T00:06:40.746039Z",
     "iopub.status.idle": "2025-01-12T00:06:40.750565Z",
     "shell.execute_reply": "2025-01-12T00:06:40.749692Z"
    },
    "papermill": {
     "duration": 0.012437,
     "end_time": "2025-01-12T00:06:40.752203",
     "exception": false,
     "start_time": "2025-01-12T00:06:40.739766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\" \n",
    "\n",
    "\n",
    "\"general\": {\n",
    "\"num_rows\": 230130,\n",
    "\"num_columns\": 6,\n",
    "\"num_missing_values\": \"8871\",\n",
    "\"percent_missing_values\": 0.6424629557206796\n",
    "},\n",
    "\"data_types\": {\n",
    "\"date\": \"object\",\n",
    "\"country\": \"object\",\n",
    "\"store\": \"object\",\n",
    "\"product\": \"object\",\n",
    "\"num_sold\": \"float64\"\n",
    "},\n",
    "\"missing_values\": {\n",
    "\"date\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"country\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"store\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"product\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"num_sold\": {\n",
    "\"missing_count\": 8871,\n",
    "\"percent_missing\": 3.8547777343240774\n",
    "}\n",
    "},\n",
    "\"numerical_summary\": {\n",
    "\"count\": {},\n",
    "\"mean\": {},\n",
    "\"std\": {},\n",
    "\"min\": {},\n",
    "\"25%\": {},\n",
    "\"50%\": {},\n",
    "\"75%\": {},\n",
    "\"max\": {}\n",
    "},\n",
    "\"categorical_summary\": {\n",
    "\"date\": {\n",
    "\"unique_counts\": 2557\n",
    "},\n",
    "\"country\": {\n",
    "\"unique_counts\": 6\n",
    "},\n",
    "\"store\": {\n",
    "\"unique_counts\": 3\n",
    "},\n",
    "\"product\": {\n",
    "\"unique_counts\": 5\n",
    "}\n",
    "},\n",
    "\"skewness_kurtosis\": {\n",
    "\"num_sold\": {\n",
    "\"skewness\": 1.415373452498392,\n",
    "\"kurtosis\": 2.6123350629213618\n",
    "}\n",
    "},\n",
    "\"correlations\": {\n",
    "\"num_sold\": {\n",
    "\"num_sold\": 1.0\n",
    "}\n",
    "},\n",
    "\"outlier_summary\": {\n",
    "\"num_sold\": {\n",
    "\"outlier_count\": 6630,\n",
    "\"percent_outliers\": 2.8809803154738627\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110036dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:40.764762Z",
     "iopub.status.busy": "2025-01-12T00:06:40.764374Z",
     "iopub.status.idle": "2025-01-12T00:06:40.768388Z",
     "shell.execute_reply": "2025-01-12T00:06:40.767371Z"
    },
    "papermill": {
     "duration": 0.011641,
     "end_time": "2025-01-12T00:06:40.769823",
     "exception": false,
     "start_time": "2025-01-12T00:06:40.758182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "\n",
    "Provide an analysis of the following EDA summary: The target variable is num_sold.\n",
    "{context}\n",
    "\n",
    "Add a comment about the missing values in the target variable: num_sold. And the implications if those are missing at random or not. \n",
    "Key Insights and Observations\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb0b929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:40.781983Z",
     "iopub.status.busy": "2025-01-12T00:06:40.781641Z",
     "iopub.status.idle": "2025-01-12T00:06:47.695306Z",
     "shell.execute_reply": "2025-01-12T00:06:47.694318Z"
    },
    "papermill": {
     "duration": 6.921705,
     "end_time": "2025-01-12T00:06:47.697038",
     "exception": false,
     "start_time": "2025-01-12T00:06:40.775333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**selected model: gpt-4o-2024-05-13**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Comment on Missing Values in the Target Variable: `num_sold`\n",
       "\n",
       "The target variable `num_sold` has 8,871 missing values, which constitutes approximately 3.85% of the total dataset. This is a significant amount of missing data that needs to be addressed before any modeling or analysis can be performed.\n",
       "\n",
       "#### Implications if Missing at Random (MAR):\n",
       "If the missing values in `num_sold` are missing at random, it means that the likelihood of a value being missing is related to some of the observed data but not the missing data itself. In this case, the missing values can be handled using imputation techniques such as mean, median, or more sophisticated methods like multiple imputation. The assumption here is that the missing data can be predicted based on the observed data.\n",
       "\n",
       "#### Implications if Not Missing at Random (NMAR):\n",
       "If the missing values are not missing at random, it means that the missingness is related to the unobserved data itself. This scenario is more problematic because it introduces bias that cannot be easily corrected. For example, if `num_sold` is missing more frequently for certain products or stores, this could skew the analysis and lead to incorrect conclusions. In such cases, it might be necessary to use more advanced techniques like modeling the missing data mechanism or even collecting more data if possible.\n",
       "\n",
       "### Key Insights and Observations\n",
       "\n",
       "1. **General Overview**:\n",
       "   - The dataset contains 230,130 rows and 6 columns.\n",
       "   - There are 8,871 missing values, which is about 0.64% of the total data.\n",
       "\n",
       "2. **Data Types**:\n",
       "   - The dataset includes both categorical (date, country, store, product) and numerical (num_sold) data types.\n",
       "\n",
       "3. **Missing Values**:\n",
       "   - The missing values are entirely in the `num_sold` column, with a missing rate of 3.85%.\n",
       "\n",
       "4. **Categorical Summary**:\n",
       "   - The `date` column has 2,557 unique values.\n",
       "   - The `country` column has 6 unique values.\n",
       "   - The `store` column has 3 unique values.\n",
       "   - The `product` column has 5 unique values.\n",
       "\n",
       "5. **Skewness and Kurtosis**:\n",
       "   - The `num_sold` variable has a skewness of 1.415, indicating a right-skewed distribution.\n",
       "   - The kurtosis is 2.612, suggesting a distribution with heavier tails than a normal distribution.\n",
       "\n",
       "6. **Correlations**:\n",
       "   - The correlation matrix shows that `num_sold` is perfectly correlated with itself, as expected.\n",
       "\n",
       "7. **Outliers**:\n",
       "   - There are 6,630 outliers in the `num_sold` column, which is about 2.88% of the data. This indicates the presence of extreme values that could affect the analysis.\n",
       "\n",
       "### Recommendations\n",
       "\n",
       "1. **Handling Missing Values**:\n",
       "   - Investigate the pattern of missingness to determine if it is MAR or NMAR.\n",
       "   - Use appropriate imputation techniques if the data is MAR.\n",
       "   - Consider advanced methods or data collection if the data is NMAR.\n",
       "\n",
       "2. **Outlier Treatment**:\n",
       "   - Analyze the outliers to understand their impact on the model.\n",
       "   - Consider using robust statistical methods or transforming the data to mitigate the effect of outliers.\n",
       "\n",
       "3. **Further Analysis**:\n",
       "   - Perform additional exploratory data analysis to understand the relationships between `num_sold` and other variables.\n",
       "   - Consider feature engineering to create new variables that might help in predicting `num_sold`.\n",
       "\n",
       "By addressing the missing values and outliers appropriately, the dataset can be prepared for more accurate and reliable modeling and analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: /kaggle/working/output_base_model.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Prepare parameters for ChatOpenAI\n",
    "model_params = {\n",
    "    \"model\": SELECTED_LLM,\n",
    "    \"api_key\": OPENAI_API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "display(Markdown(f'**selected model: {SELECTED_LLM}**'))\n",
    "\n",
    "# Conditionally set temperature if supported\n",
    "if SELECTED_LLM != ADVANCED_LLM:\n",
    "    model_params[\"temperature\"] = TEMPERATURE \n",
    "    model_params[\"max_tokens\"] = MAX_TOKENS\n",
    "\n",
    "if SELECTED_LLM == ADVANCED_LLM:\n",
    "    model_params[\"max_completion_tokens\"] = MAX_COMPLETION_TOKENS\n",
    "\n",
    "# Initialize the model with the appropriate parameters\n",
    "model = ChatOpenAI(**model_params)\n",
    "\n",
    "# Create the processing chain\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    # Invoke the chain to get the result\n",
    "    result = chain.invoke(context)\n",
    "\n",
    "    # Save both the prompt and the result to a Markdown file\n",
    "    file_path = '/kaggle/working/output_base_model.md'\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"# EDA Report\\n\\n\")\n",
    "        f.write(\"## Prompt\\n\")\n",
    "        f.write(template.format(context=context))\n",
    "        f.write(\"\\n\\n## Response\\n\")\n",
    "        f.write(result)\n",
    "\n",
    "    # Display the result as Markdown in the notebook\n",
    "    display(Markdown(result))\n",
    "\n",
    "    display(Markdown(f\"**Markdown report saved to: {file_path}**\"))\n",
    "\n",
    "except BadRequestError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73831613",
   "metadata": {
    "papermill": {
     "duration": 0.005573,
     "end_time": "2025-01-12T00:06:47.708889",
     "exception": false,
     "start_time": "2025-01-12T00:06:47.703316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"background-color:#0A0F29; font-family:'Poppins', bold; color:#E0F7FA; font-size:140%; text-align:center; border: 2px solid #00FFFF; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">LLM automated Baseline</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0f7357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:47.721735Z",
     "iopub.status.busy": "2025-01-12T00:06:47.721332Z",
     "iopub.status.idle": "2025-01-12T00:06:47.725253Z",
     "shell.execute_reply": "2025-01-12T00:06:47.724483Z"
    },
    "papermill": {
     "duration": 0.012035,
     "end_time": "2025-01-12T00:06:47.726718",
     "exception": false,
     "start_time": "2025-01-12T00:06:47.714683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 1\n",
    "MAX_TOKENS=3500\n",
    "MAX_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa254f51",
   "metadata": {
    "papermill": {
     "duration": 0.005661,
     "end_time": "2025-01-12T00:06:47.738603",
     "exception": false,
     "start_time": "2025-01-12T00:06:47.732942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Used a simplified version of [S5E1 Previous Years Baseline - No Model](https://www.kaggle.com/code/cabaxiom/s5e1-previous-years-baseline-no-model) as template for the LLM (previous best score was 0.12585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ae6396",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:47.751687Z",
     "iopub.status.busy": "2025-01-12T00:06:47.751308Z",
     "iopub.status.idle": "2025-01-12T00:06:47.757184Z",
     "shell.execute_reply": "2025-01-12T00:06:47.756354Z"
    },
    "papermill": {
     "duration": 0.014345,
     "end_time": "2025-01-12T00:06:47.758768",
     "exception": false,
     "start_time": "2025-01-12T00:06:47.744423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_script = \"\"\"\n",
    "# =========================================\n",
    "# 1. LIBRARIES & CONFIGURATION\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =========================================\n",
    "# 2. DATA LOADING\n",
    "# =========================================\n",
    "# Paths to the datasets\n",
    "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
    "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
    "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
    "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
    "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# =========================================\n",
    "# 3. PREPROCESSING & IMPUTING MISSING VALUES\n",
    "# =========================================\n",
    "\n",
    "# Read GDP per capita data\n",
    "gdp_df = pd.read_csv(GDP_PATH)\n",
    "years = [str(year) for year in range(2010, 2021)]  # 2010 to 2020 inclusive\n",
    "\n",
    "# Filter relevant countries and years from GDP data\n",
    "relevant_countries = train_df[\"country\"].unique()\n",
    "gdp_filtered = gdp_df[gdp_df[\"Country Name\"].isin(relevant_countries)]\n",
    "gdp_filtered = gdp_filtered[[\"Country Name\"] + years]\n",
    "gdp_filtered = gdp_filtered.melt(id_vars=[\"Country Name\"], var_name=\"year\", value_name=\"gdp_per_capita\")\n",
    "gdp_filtered.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
    "gdp_filtered['year'] = gdp_filtered['year'].astype(int)\n",
    "\n",
    "# Calculate GDP ratios per year\n",
    "gdp_total = gdp_filtered.groupby('year')['gdp_per_capita'].sum().reset_index().rename(columns={'gdp_per_capita': 'total_gdp'})\n",
    "gdp_filtered = gdp_filtered.merge(gdp_total, on='year', how='left')\n",
    "gdp_filtered['gdp_ratio'] = gdp_filtered['gdp_per_capita'] / gdp_filtered['total_gdp']\n",
    "gdp_filtered.drop(columns=['gdp_per_capita', 'total_gdp'], inplace=True)\n",
    "\n",
    "# Prepare train data for imputation\n",
    "train_df['year'] = train_df['date'].dt.year\n",
    "print(f\"Missing values before imputation: {train_df['num_sold'].isna().sum()}\")\n",
    "\n",
    "# Merge train data with GDP ratios\n",
    "train_df = train_df.merge(gdp_filtered, on=['country', 'year'], how='left')\n",
    "\n",
    "# Impute missing values\n",
    "# Calculate average num_sold per (date, store, product) in Norway\n",
    "norway_avg = train_df[train_df['country'] == 'Norway'].groupby(['date', 'store', 'product'])['num_sold'].mean().reset_index()\n",
    "\n",
    "# Merge Norway averages with missing entries\n",
    "missing_mask = train_df['num_sold'].isna()\n",
    "missing_entries = train_df[missing_mask].drop(columns=['num_sold'])\n",
    "missing_entries = missing_entries.merge(norway_avg, on=['date', 'store', 'product'], how='left', suffixes=('', '_norway'))\n",
    "\n",
    "# Adjust num_sold using GDP ratios\n",
    "missing_entries = missing_entries.merge(\n",
    "    gdp_filtered.rename(columns={'country': 'country_norway', 'gdp_ratio': 'gdp_ratio_norway'}),\n",
    "    left_on=['year'],\n",
    "    right_on=['year'],\n",
    "    how='left'\n",
    ")\n",
    "missing_entries = missing_entries[missing_entries['country_norway'] == 'Norway']\n",
    "\n",
    "missing_entries['num_sold_imputed'] = missing_entries['num_sold'] * (missing_entries['gdp_ratio'] / missing_entries['gdp_ratio_norway'])\n",
    "\n",
    "# Update original train_df\n",
    "train_df.loc[missing_mask, 'num_sold'] = missing_entries['num_sold_imputed'].values\n",
    "\n",
    "print(f\"Missing values after imputation: {train_df['num_sold'].isna().sum()}\")\n",
    "\n",
    "# Handle any remaining missing values manually (if any)\n",
    "remaining_missing = train_df[train_df['num_sold'].isna()]\n",
    "if not remaining_missing.empty:\n",
    "    # Assign specific values if necessary\n",
    "    train_df.loc[train_df[\"id\"] == 23719, \"num_sold\"] = 4\n",
    "    train_df.loc[train_df[\"id\"] == 207003, \"num_sold\"] = 195\n",
    "    print(f\"Missing values after manual assignment: {train_df['num_sold'].isna().sum()}\")\n",
    "\n",
    "# =========================================\n",
    "# 4. CALCULATE STORE WEIGHTS\n",
    "# =========================================\n",
    "store_weights = train_df.groupby(\"store\")[\"num_sold\"].sum() / train_df[\"num_sold\"].sum()\n",
    "store_weights = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
    "\n",
    "# =========================================\n",
    "# 5. CALCULATE PRODUCT RATIOS\n",
    "# =========================================\n",
    "# Calculate average product ratios over all years\n",
    "product_df = train_df.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
    "product_pivot = product_df.pivot(index='date', columns='product', values='num_sold')\n",
    "product_ratio = product_pivot.div(product_pivot.sum(axis=1), axis=0)\n",
    "product_ratio = product_ratio.stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
    "\n",
    "# Instead of shifting previous years, use average ratios per (month, day)\n",
    "product_ratio['month'] = product_ratio['date'].dt.month\n",
    "product_ratio['day'] = product_ratio['date'].dt.day\n",
    "avg_product_ratio = product_ratio.groupby(['month', 'day', 'product'])['product_ratio'].mean().reset_index()\n",
    "\n",
    "# =========================================\n",
    "# 6. AGGREGATE TIME SERIES\n",
    "# =========================================\n",
    "# Aggregate total sales per date\n",
    "total_sales = train_df.groupby(\"date\")[\"num_sold\"].sum().reset_index()\n",
    "total_sales[\"year\"] = total_sales[\"date\"].dt.year\n",
    "total_sales[\"month\"] = total_sales[\"date\"].dt.month\n",
    "total_sales[\"day\"] = total_sales[\"date\"].dt.day\n",
    "total_sales[\"day_of_week\"] = total_sales[\"date\"].dt.dayofweek\n",
    "\n",
    "# =========================================\n",
    "# 7. ADJUST FOR DAY OF WEEK EFFECTS\n",
    "# =========================================\n",
    "# Calculate average num_sold per day of week\n",
    "day_of_week_avg = total_sales.groupby(\"day_of_week\")['num_sold'].mean().reset_index()\n",
    "overall_avg = total_sales['num_sold'].mean()\n",
    "day_of_week_avg['day_of_week_ratio'] = day_of_week_avg['num_sold'] / overall_avg\n",
    "total_sales = total_sales.merge(day_of_week_avg[['day_of_week', 'day_of_week_ratio']], on='day_of_week', how='left')\n",
    "total_sales['adjusted_num_sold'] = total_sales['num_sold'] / total_sales['day_of_week_ratio']\n",
    "\n",
    "# =========================================\n",
    "# 8. MAKE FORECAST\n",
    "# =========================================\n",
    "# Calculate average adjusted_num_sold per (month, day)\n",
    "avg_adjusted_sales = total_sales.groupby(['month', 'day'])['adjusted_num_sold'].mean().reset_index()\n",
    "\n",
    "# Prepare test_total_sales\n",
    "test_dates = test_df['date'].drop_duplicates()\n",
    "test_total_sales = pd.DataFrame({'date': test_dates})\n",
    "test_total_sales['month'] = test_total_sales['date'].dt.month\n",
    "test_total_sales['day'] = test_total_sales['date'].dt.day\n",
    "test_total_sales['day_of_week'] = test_total_sales['date'].dt.dayofweek\n",
    "\n",
    "# Merge with average adjusted sales and day of week ratios\n",
    "test_total_sales = test_total_sales.merge(avg_adjusted_sales, on=['month', 'day'], how='left')\n",
    "test_total_sales = test_total_sales.merge(day_of_week_avg[['day_of_week', 'day_of_week_ratio']], on='day_of_week', how='left')\n",
    "\n",
    "# Handle missing adjusted_num_sold (if any) by using overall average\n",
    "test_total_sales['adjusted_num_sold'].fillna(avg_adjusted_sales['adjusted_num_sold'].mean(), inplace=True)\n",
    "\n",
    "test_total_sales['day_num_sold'] = test_total_sales['adjusted_num_sold'] * test_total_sales['day_of_week_ratio']\n",
    "\n",
    "# =========================================\n",
    "# 9. DISAGGREGATE TOTAL SALES FORECAST\n",
    "# =========================================\n",
    "# Merge test_df with test_total_sales\n",
    "test_sub_df = test_df.merge(test_total_sales[['date', 'day_num_sold']], on='date', how='left')\n",
    "\n",
    "# Add store ratios\n",
    "test_sub_df = test_sub_df.merge(store_weights, on='store', how='left')\n",
    "\n",
    "# Add country ratios\n",
    "gdp_latest_year = gdp_filtered[gdp_filtered['year'] == gdp_filtered['year'].max()]\n",
    "test_sub_df = test_sub_df.merge(gdp_latest_year[['country', 'gdp_ratio']], on='country', how='left')\n",
    "\n",
    "# Add product ratios\n",
    "test_sub_df['month'] = test_sub_df['date'].dt.month\n",
    "test_sub_df['day'] = test_sub_df['date'].dt.day\n",
    "test_sub_df = test_sub_df.merge(avg_product_ratio, on=['month', 'day', 'product'], how='left')\n",
    "\n",
    "# Handle missing product_ratio (if any) by using overall average\n",
    "overall_product_ratio = avg_product_ratio.groupby('product')['product_ratio'].mean().reset_index()\n",
    "test_sub_df = test_sub_df.merge(overall_product_ratio, on='product', how='left', suffixes=('', '_overall'))\n",
    "test_sub_df['product_ratio'].fillna(test_sub_df['product_ratio_overall'], inplace=True)\n",
    "\n",
    "# Disaggregate to get num_sold\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"gdp_ratio\"] * test_sub_df[\"product_ratio\"]\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
    "\n",
    "# =========================================\n",
    "# 10. SUBMISSION GENERATION\n",
    "# =========================================\n",
    "submission = pd.read_csv(SUBMISSION_PATH)\n",
    "submission['num_sold'] = test_sub_df['num_sold'].values\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "submission.to_csv(f\"sub_m13_{timestamp}.csv\", index=False)\n",
    "print(\"Submission saved!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32b1d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T00:06:47.772076Z",
     "iopub.status.busy": "2025-01-12T00:06:47.771723Z",
     "iopub.status.idle": "2025-01-12T00:14:49.922159Z",
     "shell.execute_reply": "2025-01-12T00:14:49.921102Z"
    },
    "papermill": {
     "duration": 482.159063,
     "end_time": "2025-01-12T00:14:49.923836",
     "exception": false,
     "start_time": "2025-01-12T00:06:47.764773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt #1: Cleaned Code ---\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import warnings\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "# For reproducibility\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "# Paths to the datasets\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_ppp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "FINAL_SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
      "\n",
      "# Load datasets\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "# Read GDP per capita data\n",
      "gdp_df = pd.read_csv(GDP_PATH)\n",
      "\n",
      "# Filter relevant countries and years from GDP data\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "years = [str(year) for year in range(2010, 2021)]  # 2010 to 2020 inclusive\n",
      "gdp_filtered = gdp_df[gdp_df[\"Country Name\"].isin(relevant_countries)]\n",
      "gdp_filtered = gdp_filtered[[\"Country Name\"] + years]\n",
      "gdp_filtered = gdp_filtered.melt(id_vars=[\"Country Name\"], var_name=\"year\", value_name=\"gdp_per_capita\")\n",
      "gdp_filtered.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "gdp_filtered['year'] = gdp_filtered['year'].astype(int)\n",
      "\n",
      "# Calculate GDP ratios per year\n",
      "gdp_total = gdp_filtered.groupby('year')['gdp_per_capita'].sum().reset_index().rename(columns={'gdp_per_capita': 'total_gdp'})\n",
      "gdp_filtered = gdp_filtered.merge(gdp_total, on='year', how='left')\n",
      "gdp_filtered['gdp_ratio'] = gdp_filtered['gdp_per_capita'] / gdp_filtered['total_gdp']\n",
      "gdp_filtered.drop(columns=['gdp_per_capita', 'total_gdp'], inplace=True)\n",
      "\n",
      "# Prepare train data for imputation\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "print(f\"Missing values before imputation: {train_df['num_sold'].isna().sum()}\")\n",
      "\n",
      "# Merge train data with GDP ratios\n",
      "train_df = train_df.merge(gdp_filtered, on=['country', 'year'], how='left')\n",
      "\n",
      "# Impute missing values using forward fill (propagating last valid observation forward to next)\n",
      "train_df['num_sold'] = train_df['num_sold'].ffill()\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df['num_sold'].isna().sum()}\")\n",
      "\n",
      "# Creating new features in test set\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "\n",
      "# Merge test data with GDP ratios\n",
      "test_df = test_df.merge(gdp_filtered, on=['country', 'year'], how='left')\n",
      "\n",
      "# Extract the target variable and drop it from the train dataframe\n",
      "y = train_df['num_sold']\n",
      "X = train_df.drop(columns=['num_sold', 'date'])\n",
      "\n",
      "# Split data into train and validation sets\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "# Train model\n",
      "model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Validate model\n",
      "val_preds = model.predict(X_val)\n",
      "val_mape = mape(y_val, val_preds)\n",
      "print(f\"Validation MAPE: {val_mape:.4f}\")\n",
      "\n",
      "# Predict on test set\n",
      "X_test = test_df.drop(columns=['date'])\n",
      "test_preds = model.predict(X_test)\n",
      "\n",
      "# Generate submission file\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_preds\n",
      "submission.to_csv(FINAL_SUBMISSION_PATH, index=False)\n",
      "print(\"Submission saved at:\", FINAL_SUBMISSION_PATH)\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 1\n",
      "Error on attempt #1:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 164, in <cell line: 134>\n",
      "    exec(cleaned_code, {}, local_ns)\n",
      "  File \"<string>\", line 77, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 2084, in __array__\n",
      "    arr = np.asarray(values, dtype=dtype)\n",
      "ValueError: could not convert string to float: 'Norway'\n",
      "\n",
      "\n",
      "--- Attempt #2: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load datasets\n",
      "train = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Check for missing values in the target variable\n",
      "if train['num_sold'].isnull().sum() > 0:\n",
      "    print(f\"Missing values in target variable: {train['num_sold'].isnull().sum()}\")\n",
      "    train = train.dropna(subset=['num_sold'])\n",
      "else:\n",
      "    print(\"No missing values in target variable.\")\n",
      "\n",
      "# Feature engineering\n",
      "train['date'] = pd.to_datetime(train['date'])\n",
      "train['year'] = train['date'].dt.year\n",
      "train['month'] = train['date'].dt.month\n",
      "train['day'] = train['date'].dt.day\n",
      "train['dayofweek'] = train['date'].dt.dayofweek\n",
      "\n",
      "test['date'] = pd.to_datetime(test['date'])\n",
      "test['year'] = test['date'].dt.year\n",
      "test['month'] = test['date'].dt.month\n",
      "test['day'] = test['date'].dt.day\n",
      "test['dayofweek'] = test['date'].dt.dayofweek\n",
      "\n",
      "# Drop unnecessary columns\n",
      "train = train.drop(columns=['date', 'row_id'])\n",
      "test = test.drop(columns=['date', 'row_id'])\n",
      "\n",
      "# Encoding categorical variables\n",
      "train = pd.get_dummies(train, columns=['country', 'store', 'product'], drop_first=True)\n",
      "test = pd.get_dummies(test, columns=['country', 'store', 'product'], drop_first=True)\n",
      "\n",
      "# Align test and train dataframes\n",
      "X = train.drop(columns=['num_sold'])\n",
      "y = train['num_sold']\n",
      "\n",
      "# Train/Test split\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Model training\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Validation and scoring\n",
      "y_val_pred = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "\n",
      "# Make predictions on the test set\n",
      "test_preds = model.predict(test)\n",
      "\n",
      "# Prepare submission file\n",
      "sample_submission['num_sold'] = test_preds\n",
      "sample_submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "print(f\"Training data shape: {train.shape}\")\n",
      "print(f\"Test data shape: {test.shape}\")\n",
      "print(f\"Submission file saved.\")\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Missing values in target variable: 8871\n",
      "Error on attempt #2:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 164, in <cell line: 134>\n",
      "    exec(cleaned_code, {}, local_ns)\n",
      "  File \"<string>\", line 33, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\", line 5344, in drop\n",
      "    return super().drop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 4711, in drop\n",
      "    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 4753, in _drop_axis\n",
      "    new_axis = axis.drop(labels, errors=errors)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 7000, in drop\n",
      "    raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n",
      "KeyError: \"['row_id'] not found in axis\"\n",
      "\n",
      "\n",
      "--- Attempt #3: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import os\n",
      "\n",
      "# Read the data\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission_df = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Display initial information\n",
      "print(\"Train Shape:\", train_df.shape)\n",
      "print(\"Test Shape:\", test_df.shape)\n",
      "print(\"Sample Submission Shape:\", sample_submission_df.shape)\n",
      "\n",
      "# Create new features\n",
      "train_df['date'] = pd.to_datetime(train_df['date'])\n",
      "test_df['date'] = pd.to_datetime(test_df['date'])\n",
      "\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "train_df['day_of_week'] = train_df['date'].dt.dayofweek\n",
      "\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "test_df['day_of_week'] = test_df['date'].dt.dayofweek\n",
      "\n",
      "# Drop original date column\n",
      "train_df = train_df.drop(columns=['date'])\n",
      "test_df = test_df.drop(columns=['date'])\n",
      "\n",
      "# One-hot encoding for categorical features\n",
      "train_df = pd.get_dummies(train_df, columns=['country', 'store', 'product'])\n",
      "test_df = pd.get_dummies(test_df, columns=['country', 'store', 'product'])\n",
      "\n",
      "# Align the columns of train and test datasets\n",
      "train_columns = train_df.columns\n",
      "test_df = test_df.reindex(columns=train_columns, fill_value=0)\n",
      "\n",
      "# Split the data\n",
      "X = train_df.drop(columns=['num_sold'])\n",
      "y = train_df['num_sold']\n",
      "\n",
      "# Check for missing values in the target variable\n",
      "if y.isna().sum() > 0:\n",
      "    print(f\"Missing values in target: {y.isna().sum()}\")\n",
      "    y = y.fillna(y.mean())  # Filling missing values with mean\n",
      "\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Feature scaling\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_val = scaler.transform(X_val)\n",
      "test_df = scaler.transform(test_df.drop(columns=['num_sold']))\n",
      "\n",
      "# Train the model\n",
      "model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Validate the model\n",
      "y_val_pred = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
      "print(\"Validation MAPE:\", val_mape)\n",
      "\n",
      "# Prepare submission\n",
      "predictions = model.predict(test_df)\n",
      "sample_submission_df['num_sold'] = predictions\n",
      "sample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "# Output shapes to track progress\n",
      "print(\"Train data shape after preprocessing:\", X_train.shape)\n",
      "print(\"Validation data shape after preprocessing:\", X_val.shape)\n",
      "print(\"Test data shape after preprocessing:\", test_df.shape)\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train Shape: (230130, 6)\n",
      "Test Shape: (98550, 5)\n",
      "Sample Submission Shape: (98550, 2)\n",
      "Missing values in target: 8871\n",
      "Validation MAPE: 0.2429982477781527\n",
      "Train data shape after preprocessing: (184104, 19)\n",
      "Validation data shape after preprocessing: (46026, 19)\n",
      "Test data shape after preprocessing: (98550, 19)\n",
      "Code executed successfully on attempt #3!\n",
      "\n",
      "Retrieved MAPE from script: 0.2429982477781527\n",
      "Error on attempt #3:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.2430 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "\n",
      "--- Attempt #4: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "\n",
      "# Read data\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Print initial dataset shapes\n",
      "print(f\"Train shape: {train_df.shape}\")\n",
      "print(f\"Test shape: {test_df.shape}\")\n",
      "\n",
      "# Handle missing values in target variable\n",
      "train_df = train_df.dropna(subset=['num_sold'])\n",
      "\n",
      "# Create new features\n",
      "train_df['date'] = pd.to_datetime(train_df['date'])\n",
      "test_df['date'] = pd.to_datetime(test_df['date'])\n",
      "\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "train_df['dayofweek'] = train_df['date'].dt.dayofweek\n",
      "\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "test_df['dayofweek'] = test_df['date'].dt.dayofweek\n",
      "\n",
      "# Drop columns that won't be used as features\n",
      "train_df = train_df.drop(columns=['date'])\n",
      "test_df = test_df.drop(columns=['date'])\n",
      "\n",
      "# One-hot encoding for categorical features\n",
      "train_df = pd.get_dummies(train_df, columns=['country', 'product', 'store'], drop_first=True)\n",
      "test_df = pd.get_dummies(test_df, columns=['country', 'product', 'store'], drop_first=True)\n",
      "\n",
      "# Align test set with train set\n",
      "test_df = test_df.reindex(columns=train_df.columns.drop('num_sold'), fill_value=0)\n",
      "\n",
      "X = train_df.drop('num_sold', axis=1)\n",
      "y = train_df['num_sold']\n",
      "\n",
      "# Split the training data\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Train model\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Validate model\n",
      "y_pred_val = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_pred_val)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "\n",
      "# Prepare the test set predictions\n",
      "test_predictions = model.predict(test_df)\n",
      "\n",
      "# Create submission file\n",
      "sample_submission['num_sold'] = test_predictions\n",
      "sample_submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "# Print final shapes and MAPE\n",
      "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Validation MAPE: 0.050839082174786304\n",
      "Sample submission shape: (98550, 2)\n",
      "Validation MAPE: 0.050839082174786304\n",
      "Code executed successfully on attempt #4!\n",
      "\n",
      "Retrieved MAPE from script: 0.050839082174786304\n",
      "Error on attempt #4:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.0508 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "\n",
      "--- Attempt #5: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Load datasets\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "submission_df = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Print initial shapes\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "# Handle missing values in the target variable\n",
      "if train_df['num_sold'].isnull().any():\n",
      "    train_df = train_df.dropna(subset=['num_sold'])\n",
      "\n",
      "# Feature engineering: Extract date features\n",
      "train_df['date'] = pd.to_datetime(train_df['date'])\n",
      "test_df['date'] = pd.to_datetime(test_df['date'])\n",
      "\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "train_df['dayofweek'] = train_df['date'].dt.dayofweek\n",
      "\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "test_df['dayofweek'] = test_df['date'].dt.dayofweek\n",
      "\n",
      "# Encode categorical features\n",
      "le_country = LabelEncoder()\n",
      "le_product = LabelEncoder()\n",
      "le_store = LabelEncoder()\n",
      "\n",
      "train_df['country'] = le_country.fit_transform(train_df['country'])\n",
      "train_df['product'] = le_product.fit_transform(train_df['product'])\n",
      "train_df['store'] = le_store.fit_transform(train_df['store'])\n",
      "\n",
      "test_df['country'] = le_country.transform(test_df['country'])\n",
      "test_df['product'] = le_product.transform(test_df['product'])\n",
      "test_df['store'] = le_store.transform(test_df['store'])\n",
      "\n",
      "# Define features and target\n",
      "X = train_df.drop(columns=['num_sold', 'date'])\n",
      "y = train_df['num_sold']\n",
      "X_test = test_df.drop(columns=['date'])\n",
      "\n",
      "# Split the training data for validation\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Train a RandomForest model\n",
      "model = RandomForestRegressor(random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Validate the model\n",
      "y_pred = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape:.4f}\")\n",
      "\n",
      "# Predict on the test set\n",
      "y_test_pred = model.predict(X_test)\n",
      "\n",
      "# Create a submission file\n",
      "submission_df['num_sold'] = y_test_pred\n",
      "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "print(\"Submission file created.\")\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Validation MAPE: 0.0509\n",
      "Submission file created.\n",
      "Code executed successfully on attempt #5!\n",
      "\n",
      "Retrieved MAPE from script: 0.05085792765213491\n",
      "Error on attempt #5:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.0509 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "\n",
      "--- Attempt #6: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "\n",
      "# Paths\n",
      "train_path = '/kaggle/input/playground-series-s5e1/train.csv'\n",
      "test_path = '/kaggle/input/playground-series-s5e1/test.csv'\n",
      "submission_path = '/kaggle/input/playground-series-s5e1/sample_submission.csv'\n",
      "output_submission_path = '/kaggle/working/submission.csv'\n",
      "\n",
      "# Load data\n",
      "train = pd.read_csv(train_path)\n",
      "test = pd.read_csv(test_path)\n",
      "sample_submission = pd.read_csv(submission_path)\n",
      "\n",
      "# Print shapes\n",
      "print(f'Train shape: {train.shape}')\n",
      "print(f'Test shape: {test.shape}')\n",
      "\n",
      "# Preprocess data\n",
      "# Feature engineering\n",
      "train['date'] = pd.to_datetime(train['date'])\n",
      "train['year'] = train['date'].dt.year\n",
      "train['month'] = train['date'].dt.month\n",
      "train['day'] = train['date'].dt.day\n",
      "train['day_of_week'] = train['date'].dt.dayofweek\n",
      "\n",
      "test['date'] = pd.to_datetime(test['date'])\n",
      "test['year'] = test['date'].dt.year\n",
      "test['month'] = test['date'].dt.month\n",
      "test['day'] = test['date'].dt.day\n",
      "test['day_of_week'] = test['date'].dt.dayofweek\n",
      "\n",
      "# Drop original date column\n",
      "train = train.drop('date', axis=1)\n",
      "test = test.drop('date', axis=1)\n",
      "\n",
      "# Handle missing values in target variable\n",
      "train = train.dropna(subset=['num_sold'])\n",
      "\n",
      "# Prepare features and target\n",
      "X = train.drop('num_sold', axis=1)\n",
      "y = train['num_sold']\n",
      "\n",
      "# Convert categorical features to numerical\n",
      "X = pd.get_dummies(X, columns=['country', 'store', 'product'])\n",
      "test = pd.get_dummies(test, columns=['country', 'store', 'product'])\n",
      "\n",
      "# Align train and test data\n",
      "X, test = X.align(test, join='left', axis=1, fill_value=0)\n",
      "\n",
      "# Split data for validation\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_val = scaler.transform(X_val)\n",
      "test = scaler.transform(test)\n",
      "\n",
      "# Train model\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Predict on validation set\n",
      "y_val_pred = model.predict(X_val)\n",
      "\n",
      "# Calculate validation MAPE\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
      "print(f'Validation MAPE: {val_mape:.4f}')\n",
      "\n",
      "# Predict on test set\n",
      "test_predictions = model.predict(test)\n",
      "\n",
      "# Create submission file\n",
      "sample_submission['num_sold'] = test_predictions\n",
      "sample_submission.to_csv(output_submission_path, index=False)\n",
      "\n",
      "print('Submission file saved!')\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Validation MAPE: 0.0508\n",
      "Submission file saved!\n",
      "Code executed successfully on attempt #6!\n",
      "\n",
      "Retrieved MAPE from script: 0.05081476346478838\n",
      "Error on attempt #6:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.0508 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "\n",
      "--- Attempt #7: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import datetime\n",
      "\n",
      "# Load datasets\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Ensure no missing values in target and features\n",
      "train_df.dropna(subset=['num_sold'], inplace=True)\n",
      "\n",
      "def create_features(df):\n",
      "    df['date'] = pd.to_datetime(df['date'])\n",
      "    df['year'] = df['date'].dt.year\n",
      "    df['month'] = df['date'].dt.month\n",
      "    df['day'] = df['date'].dt.day\n",
      "    df['dayofweek'] = df['date'].dt.dayofweek\n",
      "    df['dayofyear'] = df['date'].dt.dayofyear\n",
      "    return df\n",
      "\n",
      "# Create features\n",
      "train_df = create_features(train_df)\n",
      "test_df = create_features(test_df)\n",
      "\n",
      "# Drop columns not needed for modeling\n",
      "X = train_df.drop(columns=['num_sold', 'date', 'row_id'])\n",
      "y = train_df['num_sold']\n",
      "test_df = test_df.drop(columns=['date', 'row_id'])\n",
      "\n",
      "# Encode categorical features\n",
      "X = pd.get_dummies(X)\n",
      "test_df = pd.get_dummies(test_df)\n",
      "\n",
      "# Align columns of test set with training set\n",
      "test_df = test_df.reindex(columns=X.columns, fill_value=0)\n",
      "\n",
      "# Split the dataset into training and validation sets\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Initialize and train the model\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Predict and evaluate on validation set\n",
      "val_predictions = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
      "\n",
      "# Output validation results\n",
      "print(f'Validation MAPE: {val_mape:.4f}')\n",
      "print(f'Training set shape: {X_train.shape}')\n",
      "print(f'Validation set shape: {X_val.shape}')\n",
      "\n",
      "# Predict on test set\n",
      "test_predictions = model.predict(test_df)\n",
      "sample_submission['num_sold'] = test_predictions\n",
      "\n",
      "# Save the submission file\n",
      "sample_submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "# Ensure the script outputs MAPE correctly\n",
      "print(f'Submission file shape: {sample_submission.shape}')\n",
      "print(f'Sample of predictions: {sample_submission.head()}')\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Error on attempt #7:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 164, in <cell line: 134>\n",
      "    exec(cleaned_code, {}, local_ns)\n",
      "  File \"<string>\", line 26, in <module>\n",
      "  File \"<string>\", line 17, in create_features\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\n",
      "--- Attempt #8: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "\n",
      "# Load datasets\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Print shapes of the datasets\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "print(\"Sample Submission shape:\", sample_submission.shape)\n",
      "\n",
      "# Check for missing values in the target\n",
      "if train_df['num_sold'].isnull().any():\n",
      "    print(\"Handling missing values in `num_sold`\")\n",
      "    train_df['num_sold'] = train_df['num_sold'].fillna(train_df['num_sold'].median())\n",
      "\n",
      "# Feature Engineering\n",
      "train_df['date'] = pd.to_datetime(train_df['date'])\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "train_df['dayofweek'] = train_df['date'].dt.dayofweek\n",
      "\n",
      "test_df['date'] = pd.to_datetime(test_df['date'])\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "test_df['dayofweek'] = test_df['date'].dt.dayofweek\n",
      "\n",
      "# Drop the 'date' column\n",
      "train_df = train_df.drop(columns=['date'])\n",
      "test_df = test_df.drop(columns=['date'])\n",
      "\n",
      "# Define features and target\n",
      "X = train_df.drop(columns=['num_sold'])\n",
      "y = train_df['num_sold']\n",
      "\n",
      "# Split the training data for validation\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Preprocessing\n",
      "numeric_features = ['year', 'month', 'day', 'dayofweek']\n",
      "categorical_features = ['country', 'store', 'product']\n",
      "\n",
      "numeric_transformer = StandardScaler()\n",
      "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
      "\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        ('num', numeric_transformer, numeric_features),\n",
      "        ('cat', categorical_transformer, categorical_features)\n",
      "    ])\n",
      "\n",
      "# Model\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "\n",
      "# Pipeline\n",
      "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
      "\n",
      "# Train the model\n",
      "pipeline.fit(X_train, y_train)\n",
      "\n",
      "# Predict on validation set\n",
      "y_val_pred = pipeline.predict(X_val)\n",
      "\n",
      "# Calculate Mean Absolute Percentage Error (MAPE)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
      "print('Validation MAPE:', val_mape)\n",
      "\n",
      "# Predict on test set\n",
      "predictions = pipeline.predict(test_df)\n",
      "\n",
      "# Create submission file\n",
      "submission = sample_submission.copy()\n",
      "submission['num_sold'] = predictions\n",
      "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "print(\"Submission saved to /kaggle/working/submission.csv\")\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Sample Submission shape: (98550, 2)\n",
      "Handling missing values in `num_sold`\n",
      "Validation MAPE: 0.2241031989504834\n",
      "Submission saved to /kaggle/working/submission.csv\n",
      "Code executed successfully on attempt #8!\n",
      "\n",
      "Retrieved MAPE from script: 0.2241031989504834\n",
      "Error on attempt #8:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.2241 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "\n",
      "--- Attempt #9: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Read data\n",
      "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\n",
      "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
      "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e1/sample_submission.csv')\n",
      "\n",
      "# Display relevant information about the data\n",
      "print(f\"Training set shape: {train_df.shape}\")\n",
      "print(f\"Test set shape: {test_df.shape}\")\n",
      "\n",
      "# Feature Engineering\n",
      "def create_features(df):\n",
      "    df['date'] = pd.to_datetime(df['date'])\n",
      "    df['year'] = df['date'].dt.year\n",
      "    df['month'] = df['date'].dt.month\n",
      "    df['day'] = df['date'].dt.day\n",
      "    df['dayofweek'] = df['date'].dt.dayofweek\n",
      "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
      "    return df\n",
      "\n",
      "# Apply feature engineering\n",
      "train_df = create_features(train_df)\n",
      "test_df = create_features(test_df)\n",
      "\n",
      "# Encode categorical variables\n",
      "label_encoders = {}\n",
      "for column in ['country', 'store', 'product']:\n",
      "    label_encoders[column] = LabelEncoder()\n",
      "    train_df[column] = label_encoders[column].fit_transform(train_df[column])\n",
      "    if column in test_df.columns:\n",
      "        test_df[column] = label_encoders[column].transform(test_df[column])\n",
      "\n",
      "# Handling missing values in the target variable if any\n",
      "train_df = train_df.dropna(subset=['num_sold'])\n",
      "\n",
      "# Prepare training and validation sets\n",
      "X = train_df.drop(columns=['id', 'date', 'num_sold'])\n",
      "y = train_df['num_sold']\n",
      "\n",
      "# Split data for validation\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Model training\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Model validation\n",
      "y_val_pred = model.predict(X_val)\n",
      "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "\n",
      "# Train on the full dataset\n",
      "model.fit(X, y)\n",
      "\n",
      "# Prepare test set for prediction\n",
      "X_test = test_df.drop(columns=['id', 'date'])\n",
      "\n",
      "# Test set prediction\n",
      "test_predictions = model.predict(X_test)\n",
      "\n",
      "# Create submission file\n",
      "submission = sample_submission.copy()\n",
      "submission['num_sold'] = test_predictions\n",
      "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "# Print submission file location confirmation\n",
      "print(\"Submission file created at: /kaggle/working/submission.csv\")\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Training set shape: (230130, 6)\n",
      "Test set shape: (98550, 5)\n",
      "Error on attempt #9:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 164, in <cell line: 134>\n",
      "    exec(cleaned_code, {}, local_ns)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "  File \"<string>\", line 19, in create_features\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\n",
      "--- Attempt #10: Cleaned Code ---\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "\n",
      "# Load the datasets\n",
      "train_path = '/kaggle/input/playground-series-s5e1/train.csv'\n",
      "test_path = '/kaggle/input/playground-series-s5e1/test.csv'\n",
      "sample_submission_path = '/kaggle/input/playground-series-s5e1/sample_submission.csv'\n",
      "\n",
      "train_df = pd.read_csv(train_path)\n",
      "test_df = pd.read_csv(test_path)\n",
      "\n",
      "# Handling missing values in the target variable if any\n",
      "train_df = train_df.dropna(subset=['num_sold'])\n",
      "\n",
      "# Feature engineering\n",
      "train_df['date'] = pd.to_datetime(train_df['date'])\n",
      "test_df['date'] = pd.to_datetime(test_df['date'])\n",
      "\n",
      "train_df['year'] = train_df['date'].dt.year\n",
      "train_df['month'] = train_df['date'].dt.month\n",
      "train_df['day'] = train_df['date'].dt.day\n",
      "train_df['dayofweek'] = train_df['date'].dt.dayofweek\n",
      "\n",
      "test_df['year'] = test_df['date'].dt.year\n",
      "test_df['month'] = test_df['date'].dt.month\n",
      "test_df['day'] = test_df['date'].dt.day\n",
      "test_df['dayofweek'] = test_df['date'].dt.dayofweek\n",
      "\n",
      "# Drop 'date' column after feature engineering\n",
      "train_df = train_df.drop(columns=['date'])\n",
      "test_df = test_df.drop(columns=['date'])\n",
      "\n",
      "# Dummy encoding for categorical features\n",
      "train_df = pd.get_dummies(train_df, columns=['country', 'store', 'product'])\n",
      "test_df = pd.get_dummies(test_df, columns=['country', 'store', 'product'])\n",
      "\n",
      "# Ensure the same columns in train and test\n",
      "train_columns = set(train_df.columns)\n",
      "test_columns = set(test_df.columns)\n",
      "missing_columns = train_columns - test_columns\n",
      "for col in missing_columns:\n",
      "    test_df[col] = 0\n",
      "\n",
      "extra_columns = test_columns - train_columns\n",
      "for col in extra_columns:\n",
      "    test_df = test_df.drop(columns=[col])\n",
      "\n",
      "# Reorder test_df columns to match train_df\n",
      "test_df = test_df[train_df.drop(columns=['num_sold']).columns]\n",
      "\n",
      "# Separate features and target variable\n",
      "X = train_df.drop(columns=['num_sold'])\n",
      "y = train_df['num_sold']\n",
      "\n",
      "# Split the data into training and validation sets\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Initialize and train the model\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Predict on validation set\n",
      "val_predictions = model.predict(X_val)\n",
      "\n",
      "# Calculate Mean Absolute Percentage Error (MAPE)\n",
      "val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
      "print('Validation MAPE:', val_mape)\n",
      "\n",
      "# Predict on test set\n",
      "test_predictions = model.predict(test_df)\n",
      "\n",
      "# Prepare the submission file\n",
      "submission_df = pd.read_csv(sample_submission_path)\n",
      "submission_df['num_sold'] = test_predictions\n",
      "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
      "\n",
      "print('Submission file saved to /kaggle/working/submission.csv')\n",
      "\n",
      "--- End of Cleaned Code ---\n",
      "\n",
      "Validation MAPE: 0.05083600876399853\n",
      "Submission file saved to /kaggle/working/submission.csv\n",
      "Code executed successfully on attempt #10!\n",
      "\n",
      "Retrieved MAPE from script: 0.05083600876399853\n",
      "Error on attempt #10:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-0405c8d4bbbd>\", line 174, in <cell line: 134>\n",
      "    raise ValueError(\n",
      "ValueError: MAPE 0.0508 is above threshold 0.05. Triggering a repair attempt.\n",
      "\n",
      "Max iterations reached. Still failing or MAPE still too high. Exiting loop.\n",
      "\n",
      "Process complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# ---------------------- LangChain or similar imports ----------------------\n",
    "# Adjust as needed for your environment\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Setup your environment\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# NEW: define the metric we want to use for evaluation\n",
    "metric = \"Mean Absolute Percentage Error (MAPE)\"\n",
    "mape_threshold = 0.05  # If MAPE is above this, we continue repairs\n",
    "\n",
    "model_params = {\n",
    "    \"model\": BASE_LLM,\n",
    "    \"api_key\": OPENAI_API_KEY,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"max_tokens\": MAX_TOKENS\n",
    "}\n",
    "llm = ChatOpenAI(**model_params)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Prepare data context\n",
    "# --------------------------------------------------------------------\n",
    "train_data_path = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
    "test_data_path = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
    "submission_example_path = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
    "gdp_path = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_ppp_per_capita.csv\"\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "train_data_summary = context\n",
    "train_data_summary_json = json.dumps(train_data_summary, indent=2)\n",
    "\n",
    "target_variable = \"num_sold\"\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Prepare your initial prompt template\n",
    "# --------------------------------------------------------------------\n",
    "prompt_template = \"\"\"\n",
    "You are given the following dataset information:\n",
    "- Train data path: {train_path}\n",
    "- GDP information for feature enginering path: {gdp_path}\n",
    "- Test data path: {test_path}\n",
    "- Submission example path: {submission_example_path}\n",
    "- Train data summary: {train_summary}\n",
    "- Target variable: {target_var}\n",
    "- Path to the final submission file to generate: {submission_path}\n",
    "\n",
    "**Task**:\n",
    "Generate a complete Python script that does the following:\n",
    "1. Reads the train data from the provided path (EXACTLY use the variable {train_path}).\n",
    "2. Creates any helpful new features based on the data summary.\n",
    "3. Trains a model to predict the target variable, which is {target_var}.\n",
    "4. Uses the test data (from {test_path}) to create predictions.\n",
    "5. Generates a submission file in the path {submission_path}, \n",
    "   using {submission_example_path} if needed for reference.\n",
    "6. Print or log any relevant steps (like shape of data, MAPE, etc.) so we can see progress.\n",
    "7. Handle missing values in the target variable if necessary.\n",
    "8. Return only valid Python code, with no triple backticks or markdown fences.\n",
    "\n",
    "Important:\n",
    "- Do **not** use placeholders like 'train.csv' by itself. You must use the actual path: {train_path}, {test_path}, etc.\n",
    "- The script must be fully self-contained: from import statements to reading/writing files.\n",
    "- Do **not** wrap the code in ```python or similar.\n",
    "\n",
    "Additionally, please compute the '{metric}' during validation (or after training) and store it in a variable (e.g., val_mape). \n",
    "We'll be using '{metric}' to evaluate the model. \n",
    "If your MAPE is above 0.10, we will attempt repairs or further improvements.\n",
    "\n",
    "see below my best code template so far:\n",
    "\n",
    "{best_model_script}\n",
    "\n",
    "Begin now.\n",
    "\"\"\"\n",
    "\n",
    "chain_prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "initial_chain = chain_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Prepare a \"repair\" prompt template\n",
    "# --------------------------------------------------------------------\n",
    "repair_prompt_template = \"\"\"\n",
    "The following code was executed but caused an error or had an unsatisfactory {metric} above 0.10. \n",
    "Here is the **entire code** that was run:\n",
    "\n",
    "\n",
    "Here is the **traceback** (if any):\n",
    "\n",
    "\n",
    "**Task**: \n",
    "1. Provide a **complete** corrected Python script that still fulfills all the original requirements:\n",
    "   - Use the actual paths provided: {train_path}, {test_path}, {submission_example_path}, {submission_path}.\n",
    "   - Create new features, train a model to predict {target_var}, and save predictions to {submission_path}.\n",
    "   - Print relevant info (shapes, validation score, {metric}, etc.) so we can track progress.\n",
    "2. If there are missing values in the target variable, show how you handle them.\n",
    "3. **Do not** enclose the script in triple backticks or any markdown fences.\n",
    "4. The script should be fully self-contained (all imports, reading data, writing submission, etc.).\n",
    "5. Please ensure the final code variable storing {metric} is named val_mape (i.e., val_mape = ...).\n",
    "\n",
    "Begin now.\n",
    "\"\"\"\n",
    "\n",
    "repair_chain_prompt = ChatPromptTemplate.from_template(template=repair_prompt_template)\n",
    "repair_chain = repair_chain_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Utility function to clean out any leftover backticks (just in case)\n",
    "# --------------------------------------------------------------------\n",
    "def remove_markdown_code_fences(code_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove triple-backtick fences from code.\n",
    "    Also removes lines that start with ``` or contain them.\n",
    "    \"\"\"\n",
    "    lines = code_str.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        if \"```\" not in line:\n",
    "            cleaned.append(line)\n",
    "    return \"\\n\".join(cleaned).strip()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Iterative generation + repair loop\n",
    "# --------------------------------------------------------------------\n",
    "current_iteration = 0\n",
    "error_encountered = True\n",
    "repaired_code = None\n",
    "\n",
    "while current_iteration < MAX_ITERATIONS and error_encountered:\n",
    "    current_iteration += 1\n",
    "    \n",
    "    if current_iteration == 1:\n",
    "        # Step 1: Use the initial chain to generate code\n",
    "        result_code = initial_chain.invoke({\n",
    "            \"train_path\": train_data_path,\n",
    "            \"gdp_path\": gdp_path,\n",
    "            \"test_path\": test_data_path,\n",
    "            \"submission_example_path\": submission_example_path,\n",
    "            \"train_summary\": train_data_summary_json,\n",
    "            \"target_var\": target_variable,\n",
    "            \"submission_path\": submission_path,\n",
    "            \"metric\": metric,\n",
    "            \"best_model_script\" : best_model_script\n",
    "        })\n",
    "    else:\n",
    "        # If we already tried once, we use the \"repaired_code\" from the LLM\n",
    "        result_code = repaired_code\n",
    "\n",
    "    # Clean out triple backticks just in case\n",
    "    cleaned_code = remove_markdown_code_fences(result_code)\n",
    "    \n",
    "    print(f\"\\n--- Attempt #{current_iteration}: Cleaned Code ---\\n\")\n",
    "    print(cleaned_code)\n",
    "    print(\"\\n--- End of Cleaned Code ---\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Execute code in a local namespace so we can retrieve variables (like val_mape)\n",
    "        local_ns = {}\n",
    "        exec(cleaned_code, {}, local_ns)\n",
    "        print(f\"Code executed successfully on attempt #{current_iteration}!\\n\")\n",
    "\n",
    "        # If the generated script provides \"val_mape\" in local_ns, let's check it:\n",
    "        if \"val_mape\" in local_ns:\n",
    "            val_mape = local_ns[\"val_mape\"]\n",
    "            print(f\"Retrieved MAPE from script: {val_mape}\")\n",
    "            \n",
    "            # If MAPE is above threshold, treat it as if an \"error\" occurred\n",
    "            if val_mape > mape_threshold:\n",
    "                raise ValueError(\n",
    "                    f\"MAPE {val_mape:.4f} is above threshold {mape_threshold}. \"\n",
    "                    \"Triggering a repair attempt.\"\n",
    "                )\n",
    "        \n",
    "        # If we get here, either there's no val_mape or it's below threshold\n",
    "        error_encountered = False\n",
    "\n",
    "    except Exception as e:\n",
    "        error_encountered = True\n",
    "        error_trace = traceback.format_exc()\n",
    "        print(f\"Error on attempt #{current_iteration}:\\n{error_trace}\")\n",
    "        \n",
    "        if current_iteration < MAX_ITERATIONS:\n",
    "            # Step 2: Feed the failing code + traceback to the repair chain\n",
    "            repaired_code = repair_chain.invoke({\n",
    "                \"generated_code\": cleaned_code,\n",
    "                \"error_trace\": error_trace,\n",
    "                \"train_path\": train_data_path,\n",
    "                \"test_path\": test_data_path,\n",
    "                \"submission_example_path\": submission_example_path,\n",
    "                \"submission_path\": submission_path,\n",
    "                \"target_var\": target_variable,\n",
    "                \"metric\": metric\n",
    "            })\n",
    "        else:\n",
    "            print(\"Max iterations reached. Still failing or MAPE still too high. Exiting loop.\\n\")\n",
    "            break\n",
    "\n",
    "print(\"Process complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 2007861,
     "sourceId": 3325325,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 504.332868,
   "end_time": "2025-01-12T00:14:51.055911",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-12T00:06:26.723043",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
