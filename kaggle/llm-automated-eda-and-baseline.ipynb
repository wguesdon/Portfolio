{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6913f92e",
   "metadata": {
    "papermill": {
     "duration": 0.004774,
     "end_time": "2025-01-23T11:21:11.099788",
     "exception": false,
     "start_time": "2025-01-23T11:21:11.095014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!-- PS-S5.E01 -->\n",
    "\n",
    "<div style=\"font-family: 'Poppins'; font-weight: bold; letter-spacing: 0px; color: #FFFFFF; font-size: 500%; text-align: center; padding: 15px; background: #0A0F29; border: 8px solid #00FFFF; border-radius: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5);\">\n",
    "    LLM : EDA and code generation <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7447d",
   "metadata": {
    "papermill": {
     "duration": 0.003588,
     "end_time": "2025-01-23T11:21:11.107525",
     "exception": false,
     "start_time": "2025-01-23T11:21:11.103937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- simple example on how to use LLM for:\n",
    "    - generating EDA summaries\n",
    "    - generating code for initial baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a659689",
   "metadata": {
    "papermill": {
     "duration": 0.003576,
     "end_time": "2025-01-23T11:21:11.114848",
     "exception": false,
     "start_time": "2025-01-23T11:21:11.111272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Next steps:\n",
    "    - Include agents in the workflow\n",
    "    - Pass along a summary of the competition instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2a69e",
   "metadata": {
    "papermill": {
     "duration": 0.003451,
     "end_time": "2025-01-23T11:21:11.122041",
     "exception": false,
     "start_time": "2025-01-23T11:21:11.118590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"background-color:#0A0F29; font-family:'Poppins', bold; color:#E0F7FA; font-size:140%; text-align:center; border: 2px solid #00FFFF; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">Generate an EDA summary</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de00dffe",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:11.131105Z",
     "iopub.status.busy": "2025-01-23T11:21:11.130696Z",
     "iopub.status.idle": "2025-01-23T11:21:22.341386Z",
     "shell.execute_reply": "2025-01-23T11:21:22.340047Z"
    },
    "papermill": {
     "duration": 11.217601,
     "end_time": "2025-01-23T11:21:22.343395",
     "exception": false,
     "start_time": "2025-01-23T11:21:11.125794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.58.1\r\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting langchain-core\r\n",
      "  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting langchain-openai\r\n",
      "  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (3.7.1)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.58.1) (1.7.0)\r\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.58.1)\r\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.58.1)\r\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (2.9.2)\r\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (4.66.5)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.58.1) (4.12.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core)\r\n",
      "  Downloading langsmith-0.3.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.58.1) (3.10)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.58.1) (1.2.2)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.58.1) (2024.8.30)\r\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.58.1)\r\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\r\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.58.1)\r\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core)\r\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core)\r\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core)\r\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.58.1) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.58.1) (2.23.4)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.2.3)\r\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_openai-0.3.1-py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading langsmith-0.3.1-py3-none-any.whl (332 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.7/332.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: zstandard, orjson, jsonpatch, jiter, h11, requests-toolbelt, httpcore, httpx, openai, langsmith, langchain-core, langchain-openai\r\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 jsonpatch-1.33 langchain-core-0.3.31 langchain-openai-0.3.1 langsmith-0.3.1 openai-1.58.1 orjson-3.10.15 requests-toolbelt-1.0.0 zstandard-0.23.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.58.1 langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9275d2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:22.357392Z",
     "iopub.status.busy": "2025-01-23T11:21:22.357032Z",
     "iopub.status.idle": "2025-01-23T11:21:22.361259Z",
     "shell.execute_reply": "2025-01-23T11:21:22.360437Z"
    },
    "papermill": {
     "duration": 0.012867,
     "end_time": "2025-01-23T11:21:22.362740",
     "exception": false,
     "start_time": "2025-01-23T11:21:22.349873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_LLM = 'gpt-4o-2024-05-13'\n",
    "ADVANCED_LLM = 'o1-preview'\n",
    "SELECTED_LLM = BASE_LLM\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b432ec5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:22.376461Z",
     "iopub.status.busy": "2025-01-23T11:21:22.376090Z",
     "iopub.status.idle": "2025-01-23T11:21:24.144845Z",
     "shell.execute_reply": "2025-01-23T11:21:24.143782Z"
    },
    "papermill": {
     "duration": 1.777647,
     "end_time": "2025-01-23T11:21:24.146828",
     "exception": false,
     "start_time": "2025-01-23T11:21:22.369181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "## LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53d4db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:24.161224Z",
     "iopub.status.busy": "2025-01-23T11:21:24.160745Z",
     "iopub.status.idle": "2025-01-23T11:21:24.429484Z",
     "shell.execute_reply": "2025-01-23T11:21:24.428459Z"
    },
    "papermill": {
     "duration": 0.278156,
     "end_time": "2025-01-23T11:21:24.431576",
     "exception": false,
     "start_time": "2025-01-23T11:21:24.153420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "OPENAI_API_KEY = user_secrets.get_secret(\"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79fa23a",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:24.445302Z",
     "iopub.status.busy": "2025-01-23T11:21:24.444985Z",
     "iopub.status.idle": "2025-01-23T11:21:24.449454Z",
     "shell.execute_reply": "2025-01-23T11:21:24.448596Z"
    },
    "papermill": {
     "duration": 0.012958,
     "end_time": "2025-01-23T11:21:24.450978",
     "exception": false,
     "start_time": "2025-01-23T11:21:24.438020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\" \n",
    "\"general\": {\n",
    "\"num_rows\": 230130,\n",
    "\"num_columns\": 6,\n",
    "\"num_missing_values\": \"8871\",\n",
    "\"percent_missing_values\": 0.6424629557206796\n",
    "},\n",
    "\"data_types\": {\n",
    "\"date\": \"object\",\n",
    "\"country\": \"object\",\n",
    "\"store\": \"object\",\n",
    "\"product\": \"object\",\n",
    "\"num_sold\": \"float64\"\n",
    "},\n",
    "\"missing_values\": {\n",
    "\"date\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"country\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"store\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"product\": {\n",
    "\"missing_count\": 0,\n",
    "\"percent_missing\": 0.0\n",
    "},\n",
    "\"num_sold\": {\n",
    "\"missing_count\": 8871,\n",
    "\"percent_missing\": 3.8547777343240774\n",
    "}\n",
    "},\n",
    "\"numerical_summary\": {\n",
    "\"count\": {},\n",
    "\"mean\": {},\n",
    "\"std\": {},\n",
    "\"min\": {},\n",
    "\"25%\": {},\n",
    "\"50%\": {},\n",
    "\"75%\": {},\n",
    "\"max\": {}\n",
    "},\n",
    "\"categorical_summary\": {\n",
    "\"date\": {\n",
    "\"unique_counts\": 2557\n",
    "},\n",
    "\"country\": {\n",
    "\"unique_counts\": 6\n",
    "},\n",
    "\"store\": {\n",
    "\"unique_counts\": 3\n",
    "},\n",
    "\"product\": {\n",
    "\"unique_counts\": 5\n",
    "}\n",
    "},\n",
    "\"skewness_kurtosis\": {\n",
    "\"num_sold\": {\n",
    "\"skewness\": 1.415373452498392,\n",
    "\"kurtosis\": 2.6123350629213618\n",
    "}\n",
    "},\n",
    "\"correlations\": {\n",
    "\"num_sold\": {\n",
    "\"num_sold\": 1.0\n",
    "}\n",
    "},\n",
    "\"outlier_summary\": {\n",
    "\"num_sold\": {\n",
    "\"outlier_count\": 6630,\n",
    "\"percent_outliers\": 2.8809803154738627\n",
    "}\n",
    "}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81da8d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:24.464191Z",
     "iopub.status.busy": "2025-01-23T11:21:24.463876Z",
     "iopub.status.idle": "2025-01-23T11:21:24.467692Z",
     "shell.execute_reply": "2025-01-23T11:21:24.466771Z"
    },
    "papermill": {
     "duration": 0.01203,
     "end_time": "2025-01-23T11:21:24.469219",
     "exception": false,
     "start_time": "2025-01-23T11:21:24.457189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "Provide an analysis of the following EDA summary: The target variable is num_sold.\n",
    "{context}\n",
    "\n",
    "Add a comment about the missing values in the target variable: num_sold. And the implications if those are missing at random or not. \n",
    "Key Insights and Observations\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123301e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:24.482632Z",
     "iopub.status.busy": "2025-01-23T11:21:24.482280Z",
     "iopub.status.idle": "2025-01-23T11:21:32.395429Z",
     "shell.execute_reply": "2025-01-23T11:21:32.394450Z"
    },
    "papermill": {
     "duration": 7.921697,
     "end_time": "2025-01-23T11:21:32.397080",
     "exception": false,
     "start_time": "2025-01-23T11:21:24.475383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**selected model: gpt-4o-2024-05-13**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Comment on Missing Values in the Target Variable: `num_sold`\n",
       "\n",
       "The target variable `num_sold` has 8,871 missing values, which constitutes approximately 3.85% of the total dataset. This is a significant amount of missing data that needs to be addressed before any modeling can be performed.\n",
       "\n",
       "#### Implications if Missing at Random (MAR) or Not:\n",
       "\n",
       "1. **Missing at Random (MAR):**\n",
       "   - If the missing values in `num_sold` are missing at random, it means that the likelihood of a value being missing is related to some of the observed data but not the missing data itself. In this case, we can use techniques such as imputation based on other observed variables (e.g., country, store, product) to estimate the missing values. This would allow us to retain the entire dataset for analysis and modeling.\n",
       "\n",
       "2. **Not Missing at Random (NMAR):**\n",
       "   - If the missing values are not missing at random, it implies that the missingness is related to the unobserved data itself. For example, certain products or stores might have systematically missing sales data due to reporting issues. In this scenario, imputation might introduce bias, and it may be more appropriate to investigate the underlying reasons for the missing data or consider using models that can handle missing data without imputation.\n",
       "\n",
       "### Key Insights and Observations\n",
       "\n",
       "1. **General Overview:**\n",
       "   - The dataset contains 230,130 rows and 6 columns.\n",
       "   - There are 8,871 missing values, which account for approximately 0.64% of the total data.\n",
       "\n",
       "2. **Data Types:**\n",
       "   - The dataset consists of categorical variables (`date`, `country`, `store`, `product`) and one numerical target variable (`num_sold`).\n",
       "\n",
       "3. **Missing Values:**\n",
       "   - All missing values are in the `num_sold` column, with no missing values in the other columns.\n",
       "\n",
       "4. **Categorical Summary:**\n",
       "   - The `date` column has 2,557 unique values, indicating a wide range of dates.\n",
       "   - The `country` column has 6 unique values, suggesting sales data from 6 different countries.\n",
       "   - The `store` column has 3 unique values, indicating data from 3 different stores.\n",
       "   - The `product` column has 5 unique values, representing 5 different products.\n",
       "\n",
       "5. **Skewness and Kurtosis:**\n",
       "   - The `num_sold` variable has a skewness of 1.415, indicating a right-skewed distribution.\n",
       "   - The kurtosis of 2.612 suggests a distribution with heavier tails than a normal distribution.\n",
       "\n",
       "6. **Correlations:**\n",
       "   - The correlation matrix shows a perfect correlation of 1.0 for `num_sold` with itself, as expected.\n",
       "\n",
       "7. **Outliers:**\n",
       "   - There are 6,630 outliers in the `num_sold` variable, accounting for approximately 2.88% of the data. This indicates the presence of extreme values that may need to be addressed during data preprocessing.\n",
       "\n",
       "### Recommendations\n",
       "\n",
       "1. **Handling Missing Values:**\n",
       "   - Investigate the pattern of missing values to determine if they are MAR or NMAR.\n",
       "   - Consider imputation techniques if the missing values are MAR, or use models that can handle missing data.\n",
       "\n",
       "2. **Addressing Outliers:**\n",
       "   - Analyze the outliers to understand their impact on the model.\n",
       "   - Consider techniques such as transformation, capping, or robust modeling methods to mitigate the influence of outliers.\n",
       "\n",
       "3. **Further Analysis:**\n",
       "   - Perform additional exploratory data analysis to understand the relationships between the categorical variables and `num_sold`.\n",
       "   - Investigate potential seasonality or trends in the `date` variable that could impact sales.\n",
       "\n",
       "By addressing these key points, we can ensure a more robust and accurate analysis and modeling of the `num_sold` variable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: /kaggle/working/output_base_model.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Prepare parameters for ChatOpenAI\n",
    "model_params = {\n",
    "    \"model\": SELECTED_LLM,\n",
    "    \"api_key\": OPENAI_API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "display(Markdown(f'**selected model: {SELECTED_LLM}**'))\n",
    "\n",
    "# Conditionally set temperature if supported\n",
    "if SELECTED_LLM != ADVANCED_LLM:\n",
    "    model_params[\"temperature\"] = TEMPERATURE \n",
    "    model_params[\"max_tokens\"] = MAX_TOKENS\n",
    "\n",
    "if SELECTED_LLM == ADVANCED_LLM:\n",
    "    model_params[\"max_completion_tokens\"] = MAX_COMPLETION_TOKENS\n",
    "\n",
    "# Initialize the model with the appropriate parameters\n",
    "model = ChatOpenAI(**model_params)\n",
    "\n",
    "# Create the processing chain\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    # Invoke the chain to get the result\n",
    "    result = chain.invoke(context)\n",
    "\n",
    "    # Save both the prompt and the result to a Markdown file\n",
    "    file_path = '/kaggle/working/output_base_model.md'\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"# EDA Report\\n\\n\")\n",
    "        f.write(\"## Prompt\\n\")\n",
    "        f.write(template.format(context=context))\n",
    "        f.write(\"\\n\\n## Response\\n\")\n",
    "        f.write(result)\n",
    "\n",
    "    # Display the result as Markdown in the notebook\n",
    "    display(Markdown(result))\n",
    "\n",
    "    display(Markdown(f\"**Markdown report saved to: {file_path}**\"))\n",
    "\n",
    "except BadRequestError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61b673",
   "metadata": {
    "papermill": {
     "duration": 0.006024,
     "end_time": "2025-01-23T11:21:32.409664",
     "exception": false,
     "start_time": "2025-01-23T11:21:32.403640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"background-color:#0A0F29; font-family:'Poppins', bold; color:#E0F7FA; font-size:140%; text-align:center; border: 2px solid #00FFFF; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">LLM automated Baseline</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f605832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:32.423249Z",
     "iopub.status.busy": "2025-01-23T11:21:32.422928Z",
     "iopub.status.idle": "2025-01-23T11:21:32.426859Z",
     "shell.execute_reply": "2025-01-23T11:21:32.425899Z"
    },
    "papermill": {
     "duration": 0.012363,
     "end_time": "2025-01-23T11:21:32.428252",
     "exception": false,
     "start_time": "2025-01-23T11:21:32.415889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.5\n",
    "MAX_TOKENS=3500\n",
    "MAX_ITERATIONS = 10\n",
    "TARGET_SCORE = 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adeff8",
   "metadata": {
    "papermill": {
     "duration": 0.006104,
     "end_time": "2025-01-23T11:21:32.440958",
     "exception": false,
     "start_time": "2025-01-23T11:21:32.434854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Used a simplified version of [S5E1 Previous Years Baseline - No Model](https://www.kaggle.com/code/cabaxiom/s5e1-previous-years-baseline-no-model) as template for the LLM (previous best score was 0.12585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9574c2",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:32.455241Z",
     "iopub.status.busy": "2025-01-23T11:21:32.454913Z",
     "iopub.status.idle": "2025-01-23T11:21:32.461131Z",
     "shell.execute_reply": "2025-01-23T11:21:32.460278Z"
    },
    "papermill": {
     "duration": 0.015392,
     "end_time": "2025-01-23T11:21:32.462726",
     "exception": false,
     "start_time": "2025-01-23T11:21:32.447334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_script = \"\"\"\n",
    "# =========================================\n",
    "# 1. LIBRARIES & CONFIGURATION\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =========================================\n",
    "# 2. DATA LOADING\n",
    "# =========================================\n",
    "# Paths to the datasets\n",
    "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
    "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
    "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
    "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
    "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# =========================================\n",
    "# 3. PREPROCESSING & IMPUTING MISSING VALUES\n",
    "# =========================================\n",
    "\n",
    "# Read GDP per capita data\n",
    "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
    "years = [str(year) for year in range(2010, 2021)]  # 2010 to 2020 inclusive\n",
    "\n",
    "# Prepare GDP ratios per country per year\n",
    "relevant_countries = train_df[\"country\"].unique()\n",
    "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
    "for year in years:\n",
    "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
    "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
    "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
    "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
    "\n",
    "# Impute missing values in train_df\n",
    "train_df_imputed = train_df.copy()\n",
    "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
    "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "for year in train_df_imputed['year'].unique():\n",
    "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
    "    # For Canada\n",
    "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
    "    ratio_can = current_ratio_canada / target_ratio\n",
    "    # Impute for Canada\n",
    "    combinations_canada = [\n",
    "        ('Discount Stickers', 'Holographic Goose'),\n",
    "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
    "        ('Stickers for Less', 'Holographic Goose')\n",
    "    ]\n",
    "    for store, product in combinations_canada:\n",
    "        mask_missing = (train_df_imputed['country'] == 'Canada') & \\\n",
    "                       (train_df_imputed['store'] == store) & \\\n",
    "                       (train_df_imputed['product'] == product) & \\\n",
    "                       (train_df_imputed['year'] == year) & \\\n",
    "                       (train_df_imputed['num_sold'].isna())\n",
    "        if not mask_missing.any():\n",
    "            continue\n",
    "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
    "        mask_norway = (train_df_imputed['country'] == 'Norway') & \\\n",
    "                      (train_df_imputed['store'] == store) & \\\n",
    "                      (train_df_imputed['product'] == product) & \\\n",
    "                      (train_df_imputed['year'] == year) & \\\n",
    "                      (train_df_imputed['date'].isin(corresponding_dates))\n",
    "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
    "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
    "\n",
    "    # For Kenya\n",
    "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
    "    ratio_ken = current_ratio_kenya / target_ratio\n",
    "    combinations_kenya = [\n",
    "        ('Discount Stickers', 'Holographic Goose'),\n",
    "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
    "        ('Stickers for Less', 'Holographic Goose'),\n",
    "        ('Discount Stickers', 'Kerneler')\n",
    "    ]\n",
    "    for store, product in combinations_kenya:\n",
    "        mask_missing = (train_df_imputed['country'] == 'Kenya') & \\\n",
    "                       (train_df_imputed['store'] == store) & \\\n",
    "                       (train_df_imputed['product'] == product) & \\\n",
    "                       (train_df_imputed['year'] == year) & \\\n",
    "                       (train_df_imputed['num_sold'].isna())\n",
    "        if not mask_missing.any():\n",
    "            continue\n",
    "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
    "        mask_norway = (train_df_imputed['country'] == 'Norway') & \\\n",
    "                      (train_df_imputed['store'] == store) & \\\n",
    "                      (train_df_imputed['product'] == product) & \\\n",
    "                      (train_df_imputed['year'] == year) & \\\n",
    "                      (train_df_imputed['date'].isin(corresponding_dates))\n",
    "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
    "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
    "\n",
    "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "# Handle any remaining missing values manually (if any)\n",
    "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
    "if not remaining_missing.empty:\n",
    "    # Assign specific values if necessary\n",
    "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
    "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
    "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "# =========================================\n",
    "# 4. CALCULATE STORE WEIGHTS\n",
    "# =========================================\n",
    "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
    "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
    "\n",
    "# =========================================\n",
    "# 5. CALCULATE PRODUCT RATIOS\n",
    "# =========================================\n",
    "# Calculate daily product ratios\n",
    "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
    "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
    "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
    "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
    "\n",
    "# Prepare forecasted product ratios by shifting previous years\n",
    "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
    "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
    "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
    "\n",
    "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
    "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
    "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=4)\n",
    "\n",
    "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
    "\n",
    "# =========================================\n",
    "# 6. AGGREGATE TIME SERIES\n",
    "# =========================================\n",
    "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
    "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
    "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
    "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
    "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
    "\n",
    "# =========================================\n",
    "# 7. ADJUST FOR DAY OF WEEK EFFECTS\n",
    "# =========================================\n",
    "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
    "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
    "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
    "\n",
    "# =========================================\n",
    "# 8. MAKE FORECAST\n",
    "# =========================================\n",
    "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
    "\n",
    "# Prepare test_total_sales_df\n",
    "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
    "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
    "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
    "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
    "\n",
    "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
    "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
    "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
    "\n",
    "# =========================================\n",
    "# 9. DISAGGREGATE TOTAL SALES FORECAST\n",
    "# =========================================\n",
    "# Merge test_df with test_total_sales_df\n",
    "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
    "\n",
    "# Add store ratios\n",
    "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
    "\n",
    "# Add country ratios\n",
    "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
    "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
    "\n",
    "# Add product ratios\n",
    "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
    "\n",
    "# Disaggregate to get num_sold\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
    "\n",
    "# =========================================\n",
    "# 10. SUBMISSION GENERATION\n",
    "# =========================================\n",
    "submission = pd.read_csv(SUBMISSION_PATH)\n",
    "submission['num_sold'] = test_sub_df['num_sold']\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "submission.to_csv(f\"sub_m13_{timestamp}.csv\", index=False)\n",
    "print(\"Submission saved!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0349fc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:21:32.476995Z",
     "iopub.status.busy": "2025-01-23T11:21:32.476673Z",
     "iopub.status.idle": "2025-01-23T11:27:29.366994Z",
     "shell.execute_reply": "2025-01-23T11:27:29.365850Z"
    },
    "papermill": {
     "duration": 356.899733,
     "end_time": "2025-01-23T11:27:29.368979",
     "exception": false,
     "start_time": "2025-01-23T11:21:32.469246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt #1 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=4)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #2 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=4)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #3 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #4 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #5 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #6 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #7 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #8 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #9 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "--- Attempt #10 ---\n",
      "--- Generated/Corrected Code Start ---\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_percentage_error\n",
      "import warnings\n",
      "\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "\n",
      "RANDOM_SEED = 42\n",
      "np.random.seed(RANDOM_SEED)\n",
      "\n",
      "TRAIN_PATH = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
      "TEST_PATH = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
      "GDP_PATH = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
      "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
      "\n",
      "train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"date\"])\n",
      "test_df = pd.read_csv(TEST_PATH, parse_dates=[\"date\"])\n",
      "print(\"Train shape:\", train_df.shape)\n",
      "print(\"Test shape:\", test_df.shape)\n",
      "\n",
      "gdp_per_capita_df = pd.read_csv(GDP_PATH)\n",
      "years = [str(year) for year in range(2010, 2021)]\n",
      "\n",
      "relevant_countries = train_df[\"country\"].unique()\n",
      "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(relevant_countries), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
      "for year in years:\n",
      "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df[year].sum()\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[f\"{year}_ratio\" for year in years]]\n",
      "gdp_per_capita_filtered_ratios_df.columns = [int(year) for year in years]\n",
      "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.stack().reset_index().rename(columns={\"level_1\": \"year\", 0: \"ratio\"})\n",
      "gdp_per_capita_filtered_ratios_df['year'] = gdp_per_capita_filtered_ratios_df['year'].astype(int)\n",
      "gdp_per_capita_filtered_ratios_df.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
      "\n",
      "train_df_imputed = train_df.copy()\n",
      "train_df_imputed['year'] = train_df_imputed['date'].dt.year\n",
      "print(f\"Missing values before imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "for year in train_df_imputed['year'].unique():\n",
      "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Norway'), 'ratio'].values[0]\n",
      "    current_ratio_canada = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Canada'), 'ratio'].values[0]\n",
      "    ratio_can = current_ratio_canada / target_ratio\n",
      "    combinations_canada = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose')\n",
      "    ]\n",
      "    for store, product in combinations_canada:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Canada') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_can\n",
      "\n",
      "    current_ratio_kenya = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df['year'] == year) & (gdp_per_capita_filtered_ratios_df['country'] == 'Kenya'), 'ratio'].values[0]\n",
      "    ratio_ken = current_ratio_kenya / target_ratio\n",
      "    combinations_kenya = [\n",
      "        ('Discount Stickers', 'Holographic Goose'),\n",
      "        ('Premium Sticker Mart', 'Holographic Goose'),\n",
      "        ('Stickers for Less', 'Holographic Goose'),\n",
      "        ('Discount Stickers', 'Kerneler')\n",
      "    ]\n",
      "    for store, product in combinations_kenya:\n",
      "        mask_missing = (train_df_imputed['country'] == 'Kenya') &                        (train_df_imputed['store'] == store) &                        (train_df_imputed['product'] == product) &                        (train_df_imputed['year'] == year) &                        (train_df_imputed['num_sold'].isna())\n",
      "        if not mask_missing.any():\n",
      "            continue\n",
      "        corresponding_dates = train_df_imputed.loc[mask_missing, 'date']\n",
      "        mask_norway = (train_df_imputed['country'] == 'Norway') &                       (train_df_imputed['store'] == store) &                       (train_df_imputed['product'] == product) &                       (train_df_imputed['year'] == year) &                       (train_df_imputed['date'].isin(corresponding_dates))\n",
      "        norway_num_sold = train_df_imputed.loc[mask_norway, 'num_sold']\n",
      "        if not norway_num_sold.empty:\n",
      "            train_df_imputed.loc[mask_missing, 'num_sold'] = norway_num_sold.values * ratio_ken\n",
      "\n",
      "print(f\"Missing values after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "remaining_missing = train_df_imputed[train_df_imputed['num_sold'].isna()]\n",
      "if not remaining_missing.empty:\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
      "    train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
      "    print(f\"Missing values after manual assignment: {train_df_imputed['num_sold'].isna().sum()}\")\n",
      "\n",
      "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum() / train_df_imputed[\"num_sold\"].sum()\n",
      "store_weights_df = store_weights.reset_index().rename(columns={\"num_sold\": \"store_ratio\"})\n",
      "\n",
      "product_df = train_df_imputed.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n",
      "product_pivot_df = product_df.pivot(index='date', columns='product', values='num_sold')\n",
      "product_ratio_df = product_pivot_df.apply(lambda x: x / x.sum(), axis=1).stack().reset_index().rename(columns={0: \"product_ratio\"})\n",
      "product_ratio_df['year'] = product_ratio_df['date'].dt.year\n",
      "\n",
      "product_ratio_2017_df = product_ratio_df[product_ratio_df['year'] == 2015].copy()\n",
      "product_ratio_2018_df = product_ratio_df[product_ratio_df['year'] == 2016].copy()\n",
      "product_ratio_2019_df = product_ratio_df[product_ratio_df['year'] == 2017].copy()\n",
      "\n",
      "product_ratio_2017_df['date'] = product_ratio_2017_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2018_df['date'] = product_ratio_2018_df['date'] + pd.DateOffset(years=2)\n",
      "product_ratio_2019_df['date'] = product_ratio_2019_df['date'] + pd.DateOffset(years=2)\n",
      "\n",
      "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df], ignore_index=True)\n",
      "\n",
      "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n",
      "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
      "train_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\n",
      "train_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\n",
      "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
      "\n",
      "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed[\"num_sold\"].mean()).rename(\"day_of_week_ratios\")\n",
      "train_df_imputed = train_df_imputed.merge(day_of_week_ratio, on='day_of_week', how='left')\n",
      "train_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
      "\n",
      "train_day_mean_df = train_df_imputed.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n",
      "\n",
      "test_total_sales_df = test_df[['date']].drop_duplicates()\n",
      "test_total_sales_df['month'] = test_total_sales_df['date'].dt.month\n",
      "test_total_sales_df['day'] = test_total_sales_df['date'].dt.day\n",
      "test_total_sales_df['day_of_week'] = test_total_sales_df['date'].dt.dayofweek\n",
      "\n",
      "test_total_sales_df = test_total_sales_df.merge(train_day_mean_df, on=['month', 'day'], how='left')\n",
      "test_total_sales_df = test_total_sales_df.merge(day_of_week_ratio.reset_index(), on='day_of_week', how='left')\n",
      "test_total_sales_df[\"day_num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n",
      "\n",
      "test_sub_df = test_df.merge(test_total_sales_df[['date', 'day_num_sold']], on='date', how='left')\n",
      "test_sub_df = test_sub_df.merge(store_weights_df, on='store', how='left')\n",
      "test_sub_df['year'] = test_sub_df['date'].dt.year\n",
      "test_sub_df = test_sub_df.merge(gdp_per_capita_filtered_ratios_df.rename(columns={'ratio': 'country_ratio'}), on=['country', 'year'], how='left')\n",
      "test_sub_df = test_sub_df.merge(forecasted_ratios_df[['date', 'product', 'product_ratio']], on=['date', 'product'], how='left')\n",
      "\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
      "test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].clip(lower=0)\n",
      "\n",
      "submission = pd.read_csv(SUBMISSION_PATH)\n",
      "submission['num_sold'] = test_sub_df['num_sold']\n",
      "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
      "print(\"Submission saved!\")\n",
      "\n",
      "train_features = train_df_imputed[['month', 'day', 'day_of_week']]\n",
      "train_target = train_df_imputed['adjusted_num_sold']\n",
      "\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.2, random_state=RANDOM_SEED)\n",
      "\n",
      "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_pred = model.predict(X_valid)\n",
      "val_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
      "print(f\"Validation MAPE: {val_mape}\")\n",
      "--- Generated/Corrected Code End ---\n",
      "\n",
      "Train shape: (230130, 6)\n",
      "Test shape: (98550, 5)\n",
      "Missing values before imputation: 8871\n",
      "Missing values after imputation: 2\n",
      "Missing values after manual assignment: 0\n",
      "Submission saved!\n",
      "Validation MAPE: 0.0763030967446642\n",
      "val_mape from script: 0.0763030967446642\n",
      "MAPE 0.0763030967446642 is above threshold\n",
      "\n",
      "Max iterations reached. Still failing. Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"Mean Absolute Percentage Error (MAPE)\"\n",
    "train_data_path = \"/kaggle/input/playground-series-s5e1/train.csv\"\n",
    "test_data_path = \"/kaggle/input/playground-series-s5e1/test.csv\"\n",
    "submission_example_path = \"/kaggle/input/playground-series-s5e1/sample_submission.csv\"\n",
    "gdp_path = \"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\"\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "target_variable = \"num_sold\"\n",
    "\n",
    "train_data_summary_json = json.dumps(context, indent=2)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Prepare your \"initial\" prompt\n",
    "# --------------------------------------------------------------------\n",
    "system_instructions = f\"\"\"\n",
    "You are a coding assistant. \n",
    "**IMPORTANT**: You must use the following MANDATORY CODE snippet as a baseline to improve. \n",
    "Do not remove the part using an external GDP dataset for features enginering. \n",
    "\n",
    "**IMPORTANT**: You must generate ready to run Python code. Do not add additional plain text and do not use delimiters like ```python ```\n",
    "the code must be ready to run via exec() do not add anything that would make this fails. \n",
    "\n",
    "--- BEGIN MANDATORY CODE ---\n",
    "\n",
    "{best_model_script}\n",
    "\n",
    "--- END MANDATORY CODE ---\n",
    "\n",
    "Only fix minor syntax or logic details if needed, but do not remove the code that reads and merges GDP data. \n",
    "Output valid Python code that runs end to end.\n",
    "\"\"\"\n",
    "\n",
    "user_instructions = f\"\"\"\n",
    "You are given the following dataset information:\n",
    "- Train data path: {train_data_path}\n",
    "- GDP path: {gdp_path}\n",
    "- Test data path: {test_data_path}\n",
    "- Submission example path: {submission_example_path}\n",
    "- Train data summary: {train_data_summary_json}\n",
    "- Target variable: {target_variable}\n",
    "- Path to the final submission file: {submission_path}\n",
    "\n",
    "**Task**:\n",
    "1. Incorporate the mandatory GDP snippet from the system instructions (already included above).\n",
    "2. Train a model to predict {target_variable}.\n",
    "3. Generate a valid Kaggle submission at {submission_path}.\n",
    "4. Compute the '{metric}' on a validation split and store it in 'val_mape'.\n",
    "5. Return *only* valid Python code, with no triple backticks.\n",
    "\n",
    "Begin now.\n",
    "\"\"\"\n",
    "\n",
    "initial_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_instructions),\n",
    "    HumanMessage(content=user_instructions),\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Prepare a \"repair\" prompt template\n",
    "# --------------------------------------------------------------------\n",
    "repair_prompt_template = \"\"\"\n",
    "The previous code caused an error or had unsatisfactory results. Below is the code that was generated:\n",
    "\n",
    "--- CODE START ---\n",
    "{previous_code}\n",
    "--- CODE END ---\n",
    "\n",
    "Here is the traceback or error message:\n",
    "\n",
    "--- ERROR START ---\n",
    "{error_trace}\n",
    "--- ERROR END ---\n",
    "\n",
    "**Task**:\n",
    "1. Provide a corrected Python script that STILL meets all original requirements:\n",
    "   - Must incorporate the mandatory GDP snippet (already provided in the system instructions).\n",
    "   - Must generate a submission file and compute '{metric}' in a variable named 'val_mape'.\n",
    "2. Do **not** remove or ignore the GDP logic from the snippet, only fix the necessary parts.\n",
    "3. Return only valid Python code, with no triple backticks or markdown.\n",
    "\n",
    "Begin now.\n",
    "\"\"\"\n",
    "\n",
    "repair_chain_prompt = ChatPromptTemplate.from_template(repair_prompt_template)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Create your LLM & output parser\n",
    "# --------------------------------------------------------------------\n",
    "model_params = {\n",
    "    \"model\": BASE_LLM,\n",
    "    \"openai_api_key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "if SELECTED_LLM != ADVANCED_LLM:\n",
    "    model_params[\"temperature\"] = TEMPERATURE \n",
    "    model_params[\"max_tokens\"] = MAX_TOKENS\n",
    "    \n",
    "llm = ChatOpenAI(**model_params)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Helper Function to remove triple backticks\n",
    "# --------------------------------------------------------------------\n",
    "def remove_markdown_code_fences(code_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove triple-backtick fences from code.\n",
    "    Also removes lines that contain them.\n",
    "    \"\"\"\n",
    "    lines = code_str.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        if \"```\" not in line:\n",
    "            cleaned.append(line)\n",
    "    return \"\\n\".join(cleaned).strip()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Iterative generation logic\n",
    "# --------------------------------------------------------------------\n",
    "iteration = 0\n",
    "success = False\n",
    "current_code = None\n",
    "\n",
    "while iteration < MAX_ITERATIONS and not success:\n",
    "    iteration += 1\n",
    "    print(f\"\\n--- Attempt #{iteration} ---\")\n",
    "\n",
    "    if iteration == 1:\n",
    "        # Use the initial chain\n",
    "        chain = initial_prompt_template | llm | parser\n",
    "        result_code = chain.invoke({\n",
    "            \"best_model_script\" : best_model_script,\n",
    "            \"train_path\": train_data_path,\n",
    "            \"gdp_path\": gdp_path,\n",
    "            \"test_path\": test_data_path,\n",
    "            \"submission_example_path\": submission_example_path,\n",
    "            \"train_summary\": train_data_summary_json,\n",
    "            \"target_variable\": target_variable,\n",
    "            \"submission_path\": submission_path,\n",
    "            \"metric\": metric\n",
    "        })\n",
    "    else:\n",
    "        # Use the repair chain with previous_code & error_trace\n",
    "        # That chain is basically the repair_prompt_template + system_instructions if needed\n",
    "        repair_chain = repair_chain_prompt | llm | parser\n",
    "        result_code = repair_chain.invoke({\n",
    "            \"previous_code\": current_code,\n",
    "            \"error_trace\": error_message,\n",
    "            \"metric\": metric\n",
    "        })\n",
    "    # Clean out triple backticks\n",
    "    cleaned_code = remove_markdown_code_fences(result_code)\n",
    "    current_code = cleaned_code  # store for next iteration if needed\n",
    "\n",
    "    print(\"--- Generated/Corrected Code Start ---\")\n",
    "    print(cleaned_code)\n",
    "    print(\"--- Generated/Corrected Code End ---\\n\")\n",
    "\n",
    "    # Attempt to exec the code\n",
    "    local_namespace = {}\n",
    "    try:\n",
    "        exec(cleaned_code, {}, local_namespace)\n",
    "\n",
    "        # Check if there's a val_mape and if it is numeric\n",
    "        if \"val_mape\" in local_namespace:\n",
    "            val_mape = local_namespace[\"val_mape\"]\n",
    "            print(f\"val_mape from script: {val_mape}\")\n",
    "            if val_mape < 0.07:\n",
    "                success = True\n",
    "            else:\n",
    "                error_message = f\"MAPE {val_mape} is above threshold\"\n",
    "                print(error_message)\n",
    "                continue\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        error_trace = traceback.format_exc()\n",
    "        print(\"Error encountered while running the generated code:\")\n",
    "        print(error_trace)\n",
    "        error_message = error_trace  # store for next iteration\n",
    "        # Not success, continue the loop\n",
    "\n",
    "if not success:\n",
    "    print(\"\\nMax iterations reached. Still failing. Exiting.\\n\")\n",
    "else:\n",
    "    print(\"\\nProcess completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 2007861,
     "sourceId": 3325325,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 382.291415,
   "end_time": "2025-01-23T11:27:30.503175",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-23T11:21:08.211760",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
